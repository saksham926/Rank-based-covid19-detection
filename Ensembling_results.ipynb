{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3cc4ce32",
      "metadata": {
        "id": "3cc4ce32"
      },
      "source": [
        "### without segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42acf118",
      "metadata": {
        "scrolled": true,
        "id": "42acf118"
      },
      "outputs": [],
      "source": [
        "import os,glob\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "gpu=input(\"Which gpu number you would like to allocate:\")\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpu\n",
        "\n",
        "import glob\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import re\n",
        "import datetime\n",
        "import keras\n",
        "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
        "from tensorflow.keras.layers import concatenate,Flatten,Layer,ReLU, MaxPooling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.svm import LinearSVC\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skimage.feature import hog,local_binary_pattern\n",
        "from skimage import data, exposure\n",
        "from skimage.transform import radon, rescale\n",
        "from skimage.filters import roberts, sobel, scharr, prewitt\n",
        "from classification_models.keras import Classifiers\n",
        "from skimage import feature\n",
        "import os,glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import argparse\n",
        "import re\n",
        "import datetime\n",
        "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
        "from tensorflow.keras.layers import concatenate,Flatten,ConvLSTM2D,LayerNormalization,GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Sequential\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.svm import LinearSVC\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import loadtxt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skimage.feature import hog,local_binary_pattern\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "from skimage import data, exposure\n",
        "from tensorflow.keras.layers import Layer\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def no_data_augmentation(normal_files,covid_files,pneumonia_files):\n",
        "    aug_normal=[]\n",
        "    aug_covid=[]\n",
        "    aug_pneumonia=[]\n",
        "    for ele in normal_files:\n",
        "        #ele=ele/255.0\n",
        "        x = Image.open(ele).convert('L')\n",
        "        x = asarray(x)\n",
        "        \n",
        "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        pic=pic/255.0\n",
        "        aug_normal.append(pic)\n",
        "    for ele in covid_files:\n",
        "        #ele=ele/255.0\n",
        "        x = Image.open(ele).convert('L')\n",
        "        x = asarray(x)\n",
        "        \n",
        "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        pic=pic/255.0\n",
        "        aug_covid.append(pic)\n",
        "    for ele in pneumonia_files:\n",
        "        #ele=ele/255.0\n",
        "        x = Image.open(ele).convert('L')\n",
        "        x = asarray(x)\n",
        "      \n",
        "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        pic=pic/255.0\n",
        "        aug_pneumonia.append(pic)\n",
        "    for i in range(len(aug_normal)):\n",
        "        aug_normal[i]=aug_normal[i].reshape((224,224))\n",
        "    \n",
        "    for i in range(len(aug_covid)):\n",
        "        aug_covid[i]=aug_covid[i].reshape((224,224))\n",
        "    for i in range(len(aug_pneumonia)):\n",
        "        aug_pneumonia[i]=aug_pneumonia[i].reshape((224,224))\n",
        "    \n",
        "    print(\"Normal files without augmentation:\",len(aug_normal))\n",
        "    print(\"Covid files without augmentation:\", len(aug_covid))\n",
        "    print(\"Pneumonia files without augmentation:\",len(aug_pneumonia))\n",
        "    return aug_normal,aug_covid,aug_pneumonia\n",
        "\n",
        "def data_augmentation(normal_files,covid_files,pneumonia_files):\n",
        "    aug_normal=[]\n",
        "    aug_covid=[]\n",
        "    thresh_hold=7\n",
        "    aug_pneumonia=[]\n",
        "    \n",
        "    #x = tf.keras.preprocessing.image.load_img(\"/content/IM-0001-0001.jpeg\")\n",
        "    \n",
        "    datagen=ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "\n",
        "    )\n",
        "    #normal\n",
        "    counter=0\n",
        "    \n",
        "    for location in tqdm(normal_files):\n",
        "        counter=0\n",
        "\n",
        "        x = Image.open(location).convert('L')\n",
        "        x = asarray(x)\n",
        "       \n",
        "        x=cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        x=x/255.0\n",
        "        x = np.expand_dims(x, axis=-1) \n",
        "        x=x.reshape((1,)+x.shape)\n",
        "        #x=x/255.0\n",
        "\n",
        "\n",
        "        for i in datagen.flow(x):\n",
        "            if counter>=17:\n",
        "                break\n",
        "            #i=i/255.0\n",
        "\n",
        "            #i = cv2.resize(i,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "            aug_normal.append(i)\n",
        "            counter+=1\n",
        "    #covid\n",
        "    counter=0\n",
        "    for location in tqdm(covid_files):\n",
        "        counter=0\n",
        "        x = Image.open(location).convert('L')\n",
        "        x = asarray(x)\n",
        "    \n",
        "        x=cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        x=x/255.0\n",
        "        x = np.expand_dims(x, axis=-1) \n",
        "        #x=img_to_array(x)\n",
        "        x=x.reshape((1,)+x.shape)\n",
        "        #x=x/255.0\n",
        "\n",
        "\n",
        "        for i in datagen.flow(x):\n",
        "            if counter>=2:\n",
        "                break\n",
        "\n",
        "            aug_covid.append(i)\n",
        "            counter+=1    \n",
        "    #pneumonia\n",
        "    counter=0\n",
        "    for location in tqdm(pneumonia_files):\n",
        "        counter=0\n",
        "        x = Image.open(location).convert('L')\n",
        "        x = asarray(x)\n",
        "    \n",
        "\n",
        "        x=cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        x=x/255.0\n",
        "        x = np.expand_dims(x, axis=-1) \n",
        "        #x=img_to_array(x)\n",
        "        x=x.reshape((1,)+x.shape)\n",
        "        #x=x/255.0\n",
        "\n",
        "        for i in datagen.flow(x):\n",
        "            if counter>=3:\n",
        "                break\n",
        "            #i=i/255.0\n",
        "            #i = cv2.resize(i,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "            aug_pneumonia.append(i)\n",
        "            counter+=1    \n",
        "\n",
        "    for ele in normal_files:\n",
        "        #ele=ele/255.0\n",
        "        x = Image.open(ele).convert('L')\n",
        "        x = asarray(x)\n",
        "        \n",
        "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        pic=pic/255.0\n",
        "        aug_normal.append(pic)\n",
        "    for ele in covid_files:\n",
        "        #ele=ele/255.0\n",
        "        x = Image.open(ele).convert('L')\n",
        "        x = asarray(x)\n",
        "        \n",
        "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        pic=pic/255.0\n",
        "        aug_covid.append(pic)\n",
        "    for ele in pneumonia_files:\n",
        "        #ele=ele/255.0\n",
        "        x = Image.open(ele).convert('L')\n",
        "        x = asarray(x)\n",
        "      \n",
        "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        pic=pic/255.0\n",
        "        aug_pneumonia.append(pic)\n",
        "    for i in range(len(aug_normal)):\n",
        "        aug_normal[i]=aug_normal[i].reshape((224,224))\n",
        "    \n",
        "    for i in range(len(aug_covid)):\n",
        "        aug_covid[i]=aug_covid[i].reshape((224,224))\n",
        "    for i in range(len(aug_pneumonia)):\n",
        "        aug_pneumonia[i]=aug_pneumonia[i].reshape((224,224))\n",
        "    \n",
        "    print(\"Normal files after augmentation:\",len(aug_normal))\n",
        "    print(\"Covid files after augmentation:\", len(aug_covid))\n",
        "    print(\"Pneumonia files after augmentation:\",len(aug_pneumonia))\n",
        "    return aug_normal,aug_covid,aug_pneumonia\n",
        "\n",
        "def making_full_data(aug_normal,aug_covid,aug_pneumonia):\n",
        "    aug_normal=shuffle(aug_normal, random_state=0)\n",
        "    aug_covid=shuffle(aug_covid,random_state=0)\n",
        "    aug_pneumonia=shuffle(aug_pneumonia,random_state=0)\n",
        "    \n",
        "    aug_normal_labels=[]\n",
        "    for i in range(len(aug_normal)):\n",
        "        aug_normal_labels.append(0)\n",
        "    print(np.shape(aug_normal),np.shape(aug_normal_labels))\n",
        "    aug_covid_labels=[]\n",
        "    for i in range(len(aug_covid)):\n",
        "        aug_covid_labels.append(1)\n",
        "    print(np.shape(aug_covid),np.shape(aug_covid_labels))\n",
        "    aug_pneumonia_labels=[]\n",
        "    for i in range(len(aug_pneumonia)):\n",
        "        aug_pneumonia_labels.append(2)\n",
        "    print(np.shape(aug_pneumonia),np.shape(aug_pneumonia_labels))  \n",
        "\n",
        "    full_data=[]\n",
        "    full_label=[]\n",
        "    for i in range(len(aug_normal)):\n",
        "        full_data.append(aug_normal[i])\n",
        "        full_label.append(aug_normal_labels[i])\n",
        "    for i in range(len(aug_covid)):\n",
        "        full_data.append(aug_covid[i])\n",
        "        full_label.append(aug_covid_labels[i])\n",
        "    for i in range(len(aug_pneumonia)):\n",
        "        full_data.append(aug_pneumonia[i])\n",
        "        full_label.append(aug_pneumonia_labels[i])\n",
        "        \n",
        "    full_data=np.array(full_data)\n",
        "    full_label=np.array(full_label)\n",
        "    \n",
        "    full_data=shuffle(full_data,random_state=0)\n",
        "    full_label=shuffle(full_label,random_state=0)\n",
        "    \n",
        "    return full_data,full_label\n",
        "\"\"\"Inception 2D_CNN Models in Tensorflow-Keras.\n",
        "References -\n",
        "Inception_v1 (GoogLeNet): https://arxiv.org/abs/1409.4842 [Going Deeper with Convolutions]\n",
        "Inception_v2: http://arxiv.org/abs/1512.00567 [Rethinking the Inception Architecture for Computer Vision]\n",
        "Inception_v3: http://arxiv.org/abs/1512.00567 [Rethinking the Inception Architecture for Computer Vision]\n",
        "Inception_v4: https://arxiv.org/abs/1602.07261 [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Conv_2D_Block(x, model_width, kernel, strides=(1, 1), padding=\"same\"):\n",
        "    # 2D Convolutional Block with BatchNormalization\n",
        "    x = tf.keras.layers.Conv2D(model_width, kernel, strides=strides, padding=padding, kernel_initializer=\"he_normal\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def classifier(inputs, class_number):\n",
        "    # Construct the Classifier Group\n",
        "    # inputs       : input vector\n",
        "    # class_number : number of output classes\n",
        "    out = tf.keras.layers.Dense(class_number, activation='softmax')(inputs)\n",
        "    return out\n",
        "\n",
        "\n",
        "def regressor(inputs, feature_number):\n",
        "    # Construct the Regressor Group\n",
        "    # inputs         : input vector\n",
        "    # feature_number : number of output features\n",
        "    out = tf.keras.layers.Dense(feature_number, activation='linear')(inputs)\n",
        "    return out\n",
        "\n",
        "\n",
        "def SE_Block(inputs, num_filters, ratio):\n",
        "    squeeze = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n",
        "\n",
        "    excitation = tf.keras.layers.Dense(units=num_filters/ratio)(squeeze)\n",
        "    excitation = tf.keras.layers.Activation('relu')(excitation)\n",
        "    excitation = tf.keras.layers.Dense(units=num_filters)(excitation)\n",
        "    excitation = tf.keras.layers.Activation('sigmoid')(excitation)\n",
        "    excitation = tf.keras.layers.Reshape([1, 1, num_filters])(excitation)\n",
        "\n",
        "    scale = inputs * excitation\n",
        "\n",
        "    return scale\n",
        "\n",
        "\n",
        "def Inceptionv1_Module(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB4_1, i):\n",
        "    # Inception Block i\n",
        "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1), padding='valid')\n",
        "\n",
        "    branch3x3 = Conv_2D_Block(inputs, filterB2_1, (1, 1), padding='valid')\n",
        "    branch3x3 = Conv_2D_Block(branch3x3, filterB2_2, (3, 3))\n",
        "\n",
        "    branch5x5 = Conv_2D_Block(inputs, filterB3_1, (1, 1), padding='valid')\n",
        "    branch5x5 = Conv_2D_Block(branch5x5, filterB3_2, (5, 5))\n",
        "\n",
        "    branch_pool = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
        "    out = tf.keras.layers.concatenate([branch1x1, branch3x3, branch5x5, branch_pool], axis=-1, name='Inception_Block_'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def Inceptionv2_Module(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
        "    # Inception Block i\n",
        "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
        "\n",
        "    branch3x3 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
        "    branch3x3 = Conv_2D_Block(branch3x3, filterB2_2, (3, 3))\n",
        "\n",
        "    branch3x3dbl = Conv_2D_Block(inputs, filterB3_1, (1, 1))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (3, 3))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_3, (3, 3))\n",
        "\n",
        "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
        "\n",
        "    out = tf.keras.layers.concatenate([branch1x1, branch3x3, branch3x3dbl, branch_pool], axis=-1, name='Inception_Block_'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def Inception_Module_A(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
        "    # Inception Block i\n",
        "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
        "\n",
        "    branch5x5 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
        "    branch5x5 = Conv_2D_Block(branch5x5, filterB2_2, (5, 5))\n",
        "\n",
        "    branch3x3dbl = Conv_2D_Block(inputs, filterB3_1, (1, 1))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (3, 3))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_3, (3, 3))\n",
        "\n",
        "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
        "\n",
        "    out = tf.keras.layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=-1, name='Inception_Block_A'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def Inception_Module_B(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
        "    # Inception Block i\n",
        "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
        "\n",
        "    branch7x7 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
        "    branch7x7 = Conv_2D_Block(branch7x7, filterB2_2, (1, 7))\n",
        "    branch7x7 = Conv_2D_Block(branch7x7, filterB2_2, (7, 1))\n",
        "\n",
        "    branch7x7dbl = Conv_2D_Block(inputs, filterB3_1, 1)\n",
        "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_2, (1, 7))\n",
        "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_2, (7, 1))\n",
        "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_3, (1, 7))\n",
        "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_3, (7, 1))\n",
        "\n",
        "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
        "\n",
        "    out = tf.keras.layers.concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool], axis=-1, name='Inception_Block_B'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def Inception_Module_C(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
        "    # Inception Block i\n",
        "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
        "\n",
        "    branch3x3 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
        "    branch3x3_2 = Conv_2D_Block(branch3x3, filterB2_2, (1, 3))\n",
        "    branch3x3_3 = Conv_2D_Block(branch3x3, filterB2_2, (3, 1))\n",
        "\n",
        "    branch3x3dbl = Conv_2D_Block(inputs, filterB3_1, (1, 1))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (1, 3))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (3, 1))\n",
        "    branch3x3dbl_2 = Conv_2D_Block(branch3x3dbl, filterB3_3, (1, 3))\n",
        "    branch3x3dbl_3 = Conv_2D_Block(branch3x3dbl, filterB3_3, (3, 1))\n",
        "\n",
        "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
        "\n",
        "    out = tf.keras.layers.concatenate([branch1x1, branch3x3_2, branch3x3_3, branch3x3dbl_2, branch3x3dbl_3, branch_pool], axis=-1, name='Inception_Block_C'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def Reduction_Block_A(inputs, filterB1_1, filterB1_2, filterB2_1, filterB2_2, filterB2_3, i):\n",
        "    # Reduction Block A (i)\n",
        "    branch3x3 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
        "    branch3x3 = Conv_2D_Block(branch3x3, filterB1_2, (3, 3), strides=(2, 2))\n",
        "\n",
        "    branch3x3dbl = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_2, (3, 3))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_3, (3, 3), strides=(2, 2))\n",
        "\n",
        "    branch_pool = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(inputs)\n",
        "    out = tf.keras.layers.concatenate([branch3x3, branch3x3dbl, branch_pool], axis=-1, name='Reduction_Block_'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def Reduction_Block_B(inputs, filterB1_1, filterB1_2, filterB2_1, filterB2_2, filterB2_3, i):\n",
        "    # Reduction Block B (i)\n",
        "    branch3x3 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
        "    branch3x3 = Conv_2D_Block(branch3x3, filterB1_2, (3, 3), strides=(2, 2))\n",
        "\n",
        "    branch3x3dbl = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_2, (1, 7))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_2, (7, 1))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_3, (3, 3), strides=(2, 2))\n",
        "\n",
        "    branch_pool = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(inputs)\n",
        "    out = tf.keras.layers.concatenate([branch3x3, branch3x3dbl, branch_pool], axis=-1, name='Reduction_Block_'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class SEInception:\n",
        "    def __init__(self, length, width, num_channel, num_filters, ratio=4, problem_type='Regression',\n",
        "                 output_nums=1, pooling='avg', dropout_rate=False, auxilliary_outputs=False):\n",
        "        # length: Input Signal Length\n",
        "        # model_depth: Depth of the Model\n",
        "        # model_width: Width of the Model\n",
        "        # kernel_size: Kernel or Filter Size of the Input Convolutional Layer\n",
        "        # num_channel: Number of Channels of the Input Predictor Signals\n",
        "        # problem_type: Regression or Classification\n",
        "        # output_nums: Number of Output Classes in Classification mode and output features in Regression mode\n",
        "        # pooling: Choose either 'max' for MaxPooling or 'avg' for Averagepooling\n",
        "        # dropout_rate: If turned on, some layers will be dropped out randomly based on the selected proportion\n",
        "        # auxilliary_outputs: Two extra Auxullary outputs for the Inception models, acting like Deep Supervision\n",
        "        self.length = length\n",
        "        self.width = width\n",
        "        self.num_channel = num_channel\n",
        "        self.num_filters = num_filters\n",
        "        self.ratio = ratio\n",
        "        self.problem_type = problem_type\n",
        "        self.output_nums = output_nums\n",
        "        self.pooling = pooling\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.auxilliary_outputs = auxilliary_outputs\n",
        "\n",
        "    def MLP(self, x):\n",
        "        if self.pooling == 'avg':\n",
        "            x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "        elif self.pooling == 'max':\n",
        "            x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
        "        if self.dropout_rate:\n",
        "            x = tf.keras.layers.Dropout(self.dropout_rate)(x)\n",
        "        # Final Dense Outputting Layer for the outputs\n",
        "        x = tf.keras.layers.Flatten()(x)\n",
        "        outputs = tf.keras.layers.Dense(self.output_nums, activation='linear')(x)\n",
        "        if self.problem_type == 'Classification':\n",
        "            outputs = tf.keras.layers.Dense(self.output_nums, activation='softmax')(x)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def SEInception_v1(self):\n",
        "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
        "        # Stem\n",
        "        x = Conv_2D_Block(inputs, self.num_filters, 7, strides=2)\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "        x = Conv_2D_Block(x, self.num_filters, 1, padding='valid')\n",
        "        x = Conv_2D_Block(x, self.num_filters * 3, 3)\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "\n",
        "        x = Inceptionv1_Module(x, 64, 96, 128, 16, 32, 32, 1)  # Inception Block 1\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv1_Module(x, 128, 128, 192, 32, 96, 64, 2)  # Inception Block 2\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_0 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 0\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
        "            aux_output_0 = self.MLP(aux_conv)\n",
        "\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "        x = Inceptionv1_Module(x, 192, 96, 208, 16, 48, 64, 3)  # Inception Block 3\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv1_Module(x, 160, 112, 224, 24, 64, 64, 4)  # Inception Block 4\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv1_Module(x, 128, 128, 256, 24, 64, 64, 5)  # Inception Block 5\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv1_Module(x, 112, 144, 288, 32, 64, 64, 6)  # Inception Block 6\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv1_Module(x, 256, 160, 320, 32, 128, 128, 7)  # Inception Block 7\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_1 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 1\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
        "            aux_output_1 = self.MLP(aux_conv)\n",
        "\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "        x = Inceptionv1_Module(x, 256, 160, 320, 32, 128, 128, 8)  # Inception Block 8\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv1_Module(x, 384, 192, 384, 48, 128, 128, 9)  # Inception Block 9\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        # Final Dense MLP Layer for the outputs\n",
        "        final_output = self.MLP(x)\n",
        "        # Create model.\n",
        "        model = tf.keras.Model(inputs, final_output, name='Inception_v3')\n",
        "        if self.auxilliary_outputs:\n",
        "            model = tf.keras.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v1')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def SEInception_v2(self):\n",
        "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
        "        # Stem: 56 x 64\n",
        "        x = tf.keras.layers.SeparableConv2D(self.num_filters, kernel_size=7, strides=(2, 2), depth_multiplier=1, padding='same')(inputs)\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "        x = Conv_2D_Block(x, self.num_filters * 2, 1, padding='valid')\n",
        "        x = Conv_2D_Block(x, self.num_filters * 6, 3, padding='valid')\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "\n",
        "        x = Inceptionv2_Module(x, 64, 64, 64, 64, 96, 96, 32, 1)  # Inception Block 1: 28 x 192\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv2_Module(x, 64, 64, 96, 64, 96, 96, 64, 2)  # Inception Block 2: 28 x 256\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_0 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 0\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
        "            aux_output_0 = self.MLP(aux_conv)\n",
        "\n",
        "        x = Reduction_Block_A(x, 128, 160, 64, 96, 96, 1)  # Reduction Block 1: 28 x 320\n",
        "\n",
        "        x = Inceptionv2_Module(x, 224, 64, 96, 96, 128, 128, 128, 3)  # Inception Block 3: 14 x 576\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv2_Module(x, 192, 96, 128, 96, 128, 128, 128, 4)  # Inception Block 4: 14 x 576\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv2_Module(x, 160, 128, 160, 128, 160, 160, 96, 5)  # Inception Block 5: 14 x 576\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv2_Module(x, 96, 128, 192, 160, 192, 192, 96, 6)  # Inception Block 6: 14 x 576\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_1 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 1\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 192, 1)\n",
        "            aux_output_1 = self.MLP(aux_conv)\n",
        "\n",
        "        x = Reduction_Block_A(x, 128, 192, 192, 256, 256, 2)  # Reduction Block 2: 14 x 576\n",
        "\n",
        "        x = Inceptionv2_Module(x, 352, 192, 320, 160, 224, 224, 128, 7)  # Inception Block 7: 7 x 1024\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv2_Module(x, 352, 192, 320, 192, 224, 224, 128, 8)  # Inception Block 8: 7 x 1024\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        # Final Dense MLP Layer for the outputs\n",
        "        final_output = self.MLP(x)\n",
        "        # Create model.\n",
        "        model = tf.keras.Model(inputs, final_output, name='Inception_v3')\n",
        "        if self.auxilliary_outputs:\n",
        "            model = tf.keras.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v2')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def SEInception_v3(self):\n",
        "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
        "        # Stem\n",
        "        x = Conv_2D_Block(inputs, self.num_filters, 3, strides=2, padding='valid')\n",
        "        x = Conv_2D_Block(x, self.num_filters, 3, padding='valid')\n",
        "        x = Conv_2D_Block(x, self.num_filters * 2, 3)\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "\n",
        "        x = Conv_2D_Block(x, self.num_filters * 2.5, 1, padding='valid')\n",
        "        x = Conv_2D_Block(x, self.num_filters * 6, 3, padding='valid')\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "\n",
        "        # 3x Inception-A Blocks\n",
        "        x = Inception_Module_A(x, 64, 48, 64, 64, 96, 96, 32, 1)  # Inception-A Block 1: 35 x 256\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inception_Module_A(x, 64, 48, 64, 64, 96, 96, 64, 2)  # Inception-A Block 2: 35 x 256\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inception_Module_A(x, 64, 48, 64, 64, 96, 96, 64, 3)  # Inception-A Block 3: 35 x 256\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_0 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 0\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
        "            aux_output_0 = self.MLP(aux_conv)\n",
        "\n",
        "        x = Reduction_Block_A(x, 64, 384, 64, 96, 96, 1)  # Reduction Block 1: 17 x 768\n",
        "\n",
        "        # 4x Inception-B Blocks\n",
        "        x = Inception_Module_B(x, 192, 128, 192, 128, 128, 192, 192, 1)  # Inception-B Block 1: 17 x 768\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inception_Module_B(x, 192, 160, 192, 160, 160, 192, 192, 2)  # Inception-B Block 2: 17 x 768\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inception_Module_B(x, 192, 160, 192, 160, 160, 192, 192, 3)  # Inception-B Block 3: 17 x 768\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inception_Module_B(x, 192, 192, 192, 192, 192, 192, 192, 4)  # Inception-B Block 4: 17 x 768\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_1 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 1\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 192, 1)\n",
        "            aux_output_1 = self.MLP(aux_conv)\n",
        "\n",
        "        x = Reduction_Block_B(x, 192, 320, 192, 192, 192, 2)  # Reduction Block 2: 8 x 1280\n",
        "\n",
        "        # 2x Inception-C Blocks: 8 x 2048\n",
        "        x = Inception_Module_C(x, 320, 384, 384, 448, 384, 384, 192, 1)  # Inception-C Block 1: 8 x 2048\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inception_Module_C(x, 320, 384, 384, 448, 384, 384, 192, 2)  # Inception-C Block 2: 8 x 2048\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        # Final Dense MLP Layer for the outputs\n",
        "        final_output = self.MLP(x)\n",
        "        # Create model.\n",
        "        model = tf.keras.Model(inputs, final_output, name='Inception_v3')\n",
        "        if self.auxilliary_outputs:\n",
        "            model = tf.keras.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v3')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def SEInception_v4(self):\n",
        "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
        "        # Stem\n",
        "        x = Conv_2D_Block(inputs, 32, 3, strides=2, padding='valid')\n",
        "        x = Conv_2D_Block(x, 32, 3, padding='valid')\n",
        "        x = Conv_2D_Block(x, 64, 3)\n",
        "\n",
        "        branch1 = Conv_2D_Block(x, 96, 3, strides=2, padding='valid')\n",
        "        branch2 = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "        x = tf.keras.layers.concatenate([branch1, branch2], axis=-1)\n",
        "\n",
        "        branch1 = Conv_2D_Block(x, 64, 1)\n",
        "        branch1 = Conv_2D_Block(branch1, 96, 3, padding='valid')\n",
        "        branch2 = Conv_2D_Block(x, 64, 1)\n",
        "        branch2 = Conv_2D_Block(branch2, 64, 7)\n",
        "        branch2 = Conv_2D_Block(branch2, 96, 3, padding='valid')\n",
        "        x = tf.keras.layers.concatenate([branch1, branch2], axis=-1)\n",
        "\n",
        "        branch1 = Conv_2D_Block(x, 192, 3, padding='valid')\n",
        "        branch2 = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1))(x)\n",
        "        x = tf.keras.layers.concatenate([branch1, branch2], axis=-1)\n",
        "\n",
        "        # 4x Inception-A Blocks - 35 x 256\n",
        "        for i in range(4):\n",
        "            x = Inception_Module_A(x, 96, 64, 96, 64, 96, 96, 96, i)\n",
        "            x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_0 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 0\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 96, 1)\n",
        "            aux_output_0 = self.MLP(aux_conv)\n",
        "\n",
        "        x = Reduction_Block_A(x, 64, 384, 192, 224, 256, 1)  # Reduction Block 1: 17 x 768\n",
        "\n",
        "        # 7x Inception-B Blocks - 17 x 768\n",
        "        for i in range(7):\n",
        "            x = Inception_Module_B(x, 384, 192, 256, 192, 224, 256, 128, i)\n",
        "            x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_1 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 1\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 128, 1)\n",
        "            aux_output_1 = self.MLP(aux_conv)\n",
        "\n",
        "        x = Reduction_Block_B(x, 192, 192, 256, 320, 320, 2)  # Reduction Block 2: 8 x 1280\n",
        "\n",
        "        # 3x Inception-C Blocks: 8 x 2048\n",
        "        for i in range(3):\n",
        "            x = Inception_Module_C(x, 256, 384, 512, 384, 512, 512, 256, i)\n",
        "            x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        # Final Dense MLP Layer for the outputs\n",
        "        final_output = self.MLP(x)\n",
        "        # Create model.\n",
        "        model = tf.keras.Model(inputs, final_output, name='Inception_v4')\n",
        "        if self.auxilliary_outputs:\n",
        "            model = tf.keras.layers.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v4')\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "def making_training_and_testing_data(full_data,full_label):\n",
        "    \n",
        "    \n",
        "    train_label=[]\n",
        "    for i in range(len(full_label)):\n",
        "        if full_label[i]==0:\n",
        "            train_label.append([0,1,0])\n",
        "        elif full_label[i]==1:\n",
        "            train_label.append([1,0,0])\n",
        "        elif full_label[i]==2:\n",
        "\n",
        "            train_label.append([0,0,1])\n",
        "\n",
        "    \n",
        "    full_label=np.array(train_label)\n",
        "    \n",
        "    \n",
        "    return full_data,full_label\n",
        "    \n",
        "def my_plots(history,my_model):\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    my_path=\"training and validation accuracy curve of \"+my_model+\".png\"\n",
        "    plt.savefig(my_path)\n",
        "    plt.show()\n",
        "    \n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.ylim([0, 1])\n",
        "\n",
        "    #plt.ylim([-3, 3])\n",
        "    plt.yticks(np.arange(0, 1.1, 0.25))\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    my_path=\"training and validation loss curve of \"+my_model+\".png\"\n",
        "    plt.savefig(my_path)\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "    \n",
        "def SE_Block(inputs, num_filters, ratio):\n",
        "    squeeze = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n",
        "\n",
        "    excitation = tf.keras.layers.Dense(units=num_filters/ratio)(squeeze)\n",
        "    excitation = tf.keras.layers.Activation('relu')(excitation)\n",
        "    excitation = tf.keras.layers.Dense(units=num_filters)(excitation)\n",
        "    excitation = tf.keras.layers.Activation('sigmoid')(excitation)\n",
        "    excitation = tf.keras.layers.Reshape([1, 1, num_filters])(excitation)\n",
        "\n",
        "    scale = inputs * excitation\n",
        "    return scale\n",
        "\n",
        "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
        "    s_id = 'fire' + str(fire_id) + '/'\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = 3\n",
        "    \n",
        "    x = Conv2D(squeeze, (1, 1), padding='valid')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    left = Conv2D(expand, (1, 1), padding='valid')(x)\n",
        "    left = Activation('relu')(left)\n",
        "\n",
        "    right = Conv2D(expand, (3, 3), padding='same')(x)\n",
        "    right = Activation('relu')(right)\n",
        "\n",
        "    x = concatenate([left, right], axis=channel_axis)\n",
        "    return x\n",
        "from keras.utils import get_file\n",
        "def SE_SQUEEZNET(inputs,ratio,num_of_class):\n",
        "    x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(inputs)\n",
        "    x = Activation('relu', name='relu_conv1')(x)\n",
        "    x = MaxPool2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    \n",
        "    \n",
        "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool2')(x)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    \n",
        "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    \n",
        "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    \n",
        "    \n",
        "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool4')(x)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)    \n",
        "    \n",
        "\n",
        "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)  \n",
        "\n",
        "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    output = Dense(num_of_class, activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs, [output])\n",
        "    \n",
        "    \n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':  #straight away go to this\n",
        "    normal_dir = \"\" #give your normal cases data path here\n",
        "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
        "    dir2 = os.path.join(normal_dir,\"*.jpeg\")\n",
        "    dir = os.path.join(normal_dir,\"*.jpg\")\n",
        "    normal_files = glob.glob(dir)\n",
        "    normal_1 = glob.glob(dir1)\n",
        "    normal_2 = glob.glob(dir2)\n",
        "    normal_files.extend(normal_1)\n",
        "    normal_files.extend(normal_2)\n",
        "\n",
        "    normal_dir = \"\"  #give your covid 19 cases data path here\n",
        "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
        "    dir = os.path.join(normal_dir,\"*.jpg\")\n",
        "    dir2 = os.path.join(normal_dir,\"*.jpeg\")\n",
        "    covid_files = glob.glob(dir)\n",
        "    covid_files2 = glob.glob(dir2)\n",
        "    covid_files1 = glob.glob(dir1)\n",
        "    covid_files.extend(covid_files2)\n",
        "    covid_files.extend(covid_files1)\n",
        "\n",
        "    normal_dir = \"\" #give your pneumonia cases data path here\n",
        "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
        "    dir2 = os.path.join(normal_dir,\"*.jpeg\")\n",
        "    dir = os.path.join(normal_dir,\"*.jpg\")\n",
        "    pneumonia_files = glob.glob(dir)\n",
        "    pneumonia_1 = glob.glob(dir1)\n",
        "    pneumonia_2 = glob.glob(dir2)\n",
        "    pneumonia_files.extend(pneumonia_1)\n",
        "    pneumonia_files.extend(pneumonia_2)\n",
        "\n",
        "    normal_files.sort()\n",
        "    covid_files.sort()\n",
        "    pneumonia_files.sort()\n",
        "    normal_files=shuffle(normal_files,random_state=10)\n",
        "    covid_files=shuffle(covid_files,random_state=10)\n",
        "    pneumonia_files=shuffle(pneumonia_files,random_state=10)\n",
        "    print(\"pneumonia_files:\",len(pneumonia_files))\n",
        "    print(\"covid_files:\",len(covid_files))\n",
        "    print(\"normal_files:\",len(normal_files))\n",
        "    x=(len(normal_files)+len(covid_files)+len(pneumonia_files))*0.20\n",
        "    y=(len(normal_files)+len(covid_files)+len(pneumonia_files))*0.10\n",
        "    x=int(x//3)\n",
        "    y=int(y//3)\n",
        "    print(x)\n",
        "    print(y)\n",
        "    \n",
        "    for_normal=x-200\n",
        "    for_covid=x+150\n",
        "    for_pneumonia=x+50\n",
        "    test_normal_files=normal_files[:for_normal]\n",
        "    test_covid_files=covid_files[:for_covid]\n",
        "    test_pneumonia_files=pneumonia_files[:for_pneumonia]\n",
        "    \n",
        "    val_normal_files=normal_files[for_normal:for_normal+y]\n",
        "    val_covid_files=covid_files[for_covid:for_covid+y]\n",
        "    val_pneumonia_files=pneumonia_files[for_pneumonia:for_pneumonia+y]\n",
        "    \n",
        "    train_normal_files=normal_files[for_normal+y:]\n",
        "    train_covid_files=covid_files[for_covid+y:]\n",
        "    train_pneumonia_files=pneumonia_files[for_pneumonia+y:]\n",
        "    print(\"test normal:\",len(test_normal_files))\n",
        "    print(\"test covid:\",len(test_covid_files))\n",
        "    print(\"test pneumonia:\",len(test_pneumonia_files))\n",
        "    print(\"val normal:\",len(val_normal_files))\n",
        "    print(\"val covid:\",len(val_covid_files))\n",
        "    print(\"val pneumonia:\",len(val_pneumonia_files))\n",
        "    print(\"train normal:\",len(train_normal_files))\n",
        "    print(\"train covid:\",len(train_covid_files))\n",
        "    print(\"train pneumonia:\",len(train_pneumonia_files))\n",
        "    \n",
        "    \n",
        "    \n",
        "    train_aug_normal,train_aug_covid,train_aug_pneumonia=data_augmentation(train_normal_files,train_covid_files,train_pneumonia_files)\n",
        "    test_aug_normal,test_aug_covid,test_aug_pneumonia=no_data_augmentation(test_normal_files,test_covid_files,test_pneumonia_files)\n",
        "    val_aug_normal,val_aug_covid,val_aug_pneumonia=no_data_augmentation(val_normal_files,val_covid_files,val_pneumonia_files)\n",
        "    \n",
        "    train_full_data,train_full_label=making_full_data(train_aug_normal,train_aug_covid,train_aug_pneumonia)  #getting my full data\n",
        "    test_full_data,test_full_label=making_full_data(test_aug_normal,test_aug_covid,test_aug_pneumonia)\n",
        "    val_full_data,val_full_label=making_full_data(val_aug_normal,val_aug_covid,val_aug_pneumonia)\n",
        "    \n",
        "    train_full_data,train_full_label= making_training_and_testing_data(train_full_data,train_full_label) #dividing full_data into train and test data\n",
        "    test_full_data,test_full_label=making_training_and_testing_data(test_full_data,test_full_label)\n",
        "    val_full_data,val_full_label=making_training_and_testing_data(val_full_data,val_full_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02a11661",
      "metadata": {
        "scrolled": true,
        "id": "02a11661"
      },
      "outputs": [],
      "source": [
        "from keras.utils import get_file\n",
        "import os,glob\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from classification_models.keras import Classifiers\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import pickle\n",
        "#import clahe\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import re\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sympy.solvers import solve\n",
        "from sympy import Symbol\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
        "from tensorflow.keras.layers import concatenate,Flatten,Layer,ReLU, MaxPooling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.svm import LinearSVC\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "from sklearn.metrics import f1_score\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skimage.feature import hog,local_binary_pattern\n",
        "from skimage import data, exposure\n",
        "from skimage.transform import radon, rescale\n",
        "from skimage.filters import roberts, sobel, scharr, prewitt\n",
        "from skimage import feature\n",
        "import os,glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import argparse\n",
        "import re\n",
        "import datetime\n",
        "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
        "from tensorflow.keras.layers import concatenate,Flatten,ConvLSTM2D,LayerNormalization,GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Sequential\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.svm import LinearSVC\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import loadtxt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skimage.feature import hog,local_binary_pattern\n",
        "from skimage import data, exposure\n",
        "from tensorflow.keras.layers import Layer\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "def se_incpetion_v3(X_train,X_test,X_val,y_train,y_test,y_val):\n",
        "    #pickled_model = pickle.load(open('SE_INCEPTION_V3.sav', 'rb'))\n",
        "    pickled_model = pickle.load(open('/home/pranab_2021cs25/S_Sharma/CIKM_Fuzzy_CXR_FL/SE_INCEPTION_V3_MODEL.sav', 'rb'))\n",
        "    testing_data=X_test\n",
        "    pred=pickled_model.predict(testing_data)\n",
        "    predictions = np.argmax(pred,axis = 1)\n",
        "    y_label=np.argmax(y_test,axis = 1)\n",
        "    print(\"Accuracy of SE Inception V3 is:\",accuracy_score(predictions,y_label))\n",
        "    my_pred=[]\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i]==0:\n",
        "            my_pred.append('NORMAL')\n",
        "        elif predictions[i]==1:\n",
        "            my_pred.append('COVID-19')\n",
        "        elif predictions[i]==2:\n",
        "            my_pred.append('PNEUMONIA')\n",
        "    y_labels=[]\n",
        "    for i in range(len(y_label)):\n",
        "        if y_label[i]==0:\n",
        "            y_labels.append('NORMAL')\n",
        "        elif y_label[i]==1:\n",
        "            y_labels.append('COVID-19')\n",
        "        elif y_label[i]==2:\n",
        "            y_labels.append('PNEUMONIA')\n",
        "    \n",
        "    cm=metrics.confusion_matrix(my_pred,y_labels)\n",
        "    \n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index = ['NORMAL','COVID-19','PNEUMONIA'], \n",
        "                         columns = ['NORMAL','COVID-19','PNEUMONIA'])\n",
        "    \n",
        "#     plt.figure(figsize=(3,3))\n",
        "    sns.heatmap(cm_df, annot=True,fmt='g',annot_kws={'size':20})\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Actual Values')\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.savefig(\"SE Inception V3 Confusion matrix.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"micro precision score of SE Inception V3:\",metrics.precision_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro precision score of SE Inception V3:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro recall score of SE Inception V3:\",metrics.recall_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro recall score of SE Inception V3:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro f1 score of SE Inception V3:\",f1_score(predictions, y_label, average='micro'))\n",
        "    print(\"macro f1 score of SE Inception V3:\",f1_score(predictions, y_label, average='macro'))\n",
        "    print(\"roc auc score of SE Inception V3\",roc_auc_score(y_test, pred,multi_class='ovr'))\n",
        "    #covid=[1,0,0]\n",
        "    #normal=[0,1,0]\n",
        "    #pneumonia=[0,0,1]\n",
        "    y_test_binarized=y_test\n",
        "\n",
        "\n",
        "# roc curve for classes\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    thresh ={}\n",
        "    roc_auc = dict()\n",
        "    classes=['COVID-19','NORMAL','PNEUMONIA']\n",
        "    n_class = 3\n",
        "\n",
        "    for i in range(n_class):    \n",
        "        fpr[i], tpr[i], thresh[i] = roc_curve(y_test_binarized[:,i], pred[:,i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # plotting    \n",
        "        plt.plot(fpr[i], tpr[i], linestyle='--', \n",
        "                 label='%s vs Rest (AUC=%0.2f)'%(classes[i],roc_auc[i]))\n",
        "\n",
        "    plt.plot([0,1],[0,1],'b--')\n",
        "    plt.xlim([0,1])\n",
        "    plt.ylim([0,1.05])\n",
        "    plt.title('Multiclass ROC curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.savefig(\"AUC ROC CURVE SE Inception V3.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    \n",
        "import keras\n",
        "from classification_models.keras import Classifiers\n",
        "def densenet(X_train,X_test,X_val,y_train,y_test,y_val):\n",
        "    #pickled_model = pickle.load(open('SE_RESNEXT_101.sav', 'rb'))\n",
        "    pickled_model = pickle.load(open('/home/pranab_2021cs25/S_Sharma/CIKM_Fuzzy_CXR_FL/densent_201_model.sav', 'rb'))\n",
        "    testing_data=np.stack((X_test,)*3,axis=-1)\n",
        "    \n",
        "    pred=pickled_model.predict(testing_data)\n",
        "    predictions = np.argmax(pred,axis = 1)\n",
        "    y_label=np.argmax(y_test,axis = 1)\n",
        "    print(\"Accuracy of DENSENET 201 is:\",accuracy_score(predictions,y_label))\n",
        "    my_pred=[]\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i]==0:\n",
        "            my_pred.append('NORMAL')\n",
        "        elif predictions[i]==1:\n",
        "            my_pred.append('COVID-19')\n",
        "        elif predictions[i]==2:\n",
        "            my_pred.append('PNEUMONIA')\n",
        "    y_labels=[]\n",
        "    for i in range(len(y_label)):\n",
        "        if y_label[i]==0:\n",
        "            y_labels.append('NORMAL')\n",
        "        elif y_label[i]==1:\n",
        "            y_labels.append('COVID-19')\n",
        "        elif y_label[i]==2:\n",
        "            y_labels.append('PNEUMONIA')\n",
        "    cm=metrics.confusion_matrix(my_pred,y_labels)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                     index = ['NORMAL','COVID-19','PNEUMONIA'], \n",
        "                     columns = ['NORMAL','COVID-19','PNEUMONIA'])\n",
        "#     plt.figure(figsize=(3,3))\n",
        "    sns.heatmap(cm_df, annot=True,fmt='g',annot_kws={'size':20})\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Actual Values')\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.savefig(\"DENSENET 201 Confusion matrix.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"micro precision score of DENSENET 201:\",metrics.precision_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro precision score of DENSENET 201:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro recall score of DENSENET 201:\",metrics.recall_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro recall score of DENSENET 201:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro f1 score of DENSENET 201:\",f1_score(predictions, y_label, average='micro'))\n",
        "    print(\"macro f1 score of DENSENET 201:\",f1_score(predictions, y_label, average='macro'))\n",
        "    print(\"roc auc score of DENSENET 201:\",roc_auc_score(y_test, pred,multi_class='ovr'))\n",
        "    \n",
        "    y_test_binarized=y_test\n",
        "\n",
        "\n",
        "    # roc curve for classes\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    thresh ={}\n",
        "    roc_auc = dict()\n",
        "    classes=['COVID-19','NORMAL','PNEUMONIA']\n",
        "    n_class = 3\n",
        "\n",
        "    for i in range(n_class):    \n",
        "        fpr[i], tpr[i], thresh[i] = roc_curve(y_test_binarized[:,i], pred[:,i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # plotting    \n",
        "        plt.plot(fpr[i], tpr[i], linestyle='--', \n",
        "                 label='%s vs Rest (AUC=%0.2f)'%(classes[i],roc_auc[i]))\n",
        "\n",
        "    plt.plot([0,1],[0,1],'b--')\n",
        "    plt.xlim([0,1])\n",
        "    plt.ylim([0,1.05])\n",
        "    plt.title('Multiclass ROC curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.savefig(\"AUC ROC CURVE DENSENET 201.png\")\n",
        "    plt.show()\n",
        "\n",
        "def se_squeeznet(X_train,X_test,X_val,y_train,y_test,y_val):\n",
        "    #pickled_model = pickle.load(open('normal_densent_201.sav', 'rb'))\n",
        "       \n",
        "    pickled_model = pickle.load(open('/home/pranab_2021cs25/S_Sharma/CIKM_Fuzzy_CXR_FL/se_squeeznet_MODEL.sav', 'rb'))\n",
        "    \n",
        "    pred=pickled_model.predict(X_test)\n",
        "    predictions = np.argmax(pred,axis = 1)\n",
        "    y_label=np.argmax(y_test,axis = 1)\n",
        "    print(\"Accuracy of se_squeeznet is:\",accuracy_score(predictions,y_label))\n",
        "    my_pred=[]\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i]==0:\n",
        "            my_pred.append('NORMAL')\n",
        "        elif predictions[i]==1:\n",
        "            my_pred.append('COVID-19')\n",
        "        elif predictions[i]==2:\n",
        "            my_pred.append('PNEUMONIA')\n",
        "    y_labels=[]\n",
        "    for i in range(len(y_label)):\n",
        "        if y_label[i]==0:\n",
        "            y_labels.append('NORMAL')\n",
        "        elif y_label[i]==1:\n",
        "            y_labels.append('COVID-19')\n",
        "        elif y_label[i]==2:\n",
        "            y_labels.append('PNEUMONIA')\n",
        "    \n",
        "    cm=metrics.confusion_matrix(my_pred,y_labels)\n",
        "    \n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index = ['NORMAL','COVID-19','PNEUMONIA'], \n",
        "                         columns = ['NORMAL','COVID-19','PNEUMONIA'])\n",
        "    \n",
        "#     plt.figure(figsize=(3,3))\n",
        "    sns.heatmap(cm_df, annot=True,fmt='g',annot_kws={'size':20})\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Actual Values')\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.savefig(\"se_squeeznet Confusion matrix journal 3.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"micro precision score of se_squeeznet:\",metrics.precision_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro precision score of se_squeeznet:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro recall score of se_squeeznet:\",metrics.recall_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro recall score of se_squeeznet:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro f1 score of se_squeeznet:\",f1_score(predictions, y_label, average='micro'))\n",
        "    print(\"macro f1 score of se_squeeznet:\",f1_score(predictions, y_label, average='macro'))\n",
        "    print(\"roc auc score of se_squeeznet:\",roc_auc_score(y_test, pred,multi_class='ovr'))\n",
        "    #covid=[1,0,0]\n",
        "    #normal=[0,1,0]\n",
        "    #pneumonia=[0,0,1]\n",
        "    y_test_binarized=y_test\n",
        "\n",
        "\n",
        "# roc curve for classes\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    thresh ={}\n",
        "    roc_auc = dict()\n",
        "    classes=['COVID-19','NORMAL','PNEUMONIA']\n",
        "    n_class = 3\n",
        "\n",
        "    for i in range(n_class):    \n",
        "        fpr[i], tpr[i], thresh[i] = roc_curve(y_test_binarized[:,i], pred[:,i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # plotting    \n",
        "        plt.plot(fpr[i], tpr[i], linestyle='--', \n",
        "                 label='%s vs Rest (AUC=%0.2f)'%(classes[i],roc_auc[i]))\n",
        "\n",
        "    plt.plot([0,1],[0,1],'b--')\n",
        "    plt.xlim([0,1])\n",
        "    plt.ylim([0,1.05])\n",
        "    plt.title('Multiclass ROC curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.savefig(\"AUC ROC CURVE se_squeeznet.png\")\n",
        "    plt.show()\n",
        "\n",
        "#Fuzzy Rank-based Ensemble:\n",
        "def getScore(model,test_imgs):\n",
        "  res = model.predict(test_imgs)\n",
        "  return res \n",
        "\n",
        "def generateRank1(score,class_no):\n",
        "  rank = np.zeros([class_no,1])\n",
        "  scores = np.zeros([class_no,1])\n",
        "  scores = score\n",
        "  for i in range(class_no):\n",
        "      rank[i] = 1 - np.exp(-((scores[i]-1)**2)/2.0)\n",
        "  return rank\n",
        "\n",
        "def generateRank2(score,class_no):\n",
        "  rank = np.zeros([class_no,1])\n",
        "  scores = np.zeros([class_no,1])\n",
        "  scores = score\n",
        "  for i in range(class_no):\n",
        "      rank[i] = 1 - np.tanh(((scores[i]-1)**2)/2)\n",
        "  return rank\n",
        "\n",
        "def doFusion(res1,res2,res3,label,class_no):\n",
        "  cnt = 0\n",
        "  id = []\n",
        "  for i in range(len(res1)):\n",
        "      rank1 = generateRank1(res1[i],class_no)*generateRank2(res1[i],class_no)\n",
        "      rank2 = generateRank1(res2[i],class_no)*generateRank2(res2[i],class_no)\n",
        "      rank3 = generateRank1(res3[i],class_no)*generateRank2(res3[i],class_no)\n",
        "      rankSum = rank1 + rank2 + rank3 #list\n",
        "      rankSum = np.array(rankSum)\n",
        "      cls = np.argmin(rankSum)\n",
        "      if cls<class_no and label[i][cls]== 1:\n",
        "          cnt += 1\n",
        "      id.append(cls)\n",
        "  print(cnt/len(res1))\n",
        "  return id   \n",
        "    \n",
        "def rank_fuzzy(X_train,X_test,X_val,y_train,y_test,y_val):\n",
        "    model1 = pickle.load(open('/home/pranab_2021cs25/S_Sharma/CIKM_Fuzzy_CXR_FL/SE_INCEPTION_V3_MODEL.sav', 'rb'))\n",
        "    model2 = pickle.load(open('/home/pranab_2021cs25/S_Sharma/CIKM_Fuzzy_CXR_FL/densent_201_model.sav', 'rb'))\n",
        "    model3 = pickle.load(open('/home/pranab_2021cs25/S_Sharma/CIKM_Fuzzy_CXR_FL/se_squeeznet_MODEL.sav', 'rb'))\n",
        "    testing_data=X_test\n",
        "    res1 = model1.predict(testing_data)\n",
        "    num_classes=3\n",
        "    testing_data=np.stack((X_test,)*3,axis=-1)\n",
        "    \n",
        "    res2 = model2.predict(testing_data)\n",
        "    testing_data=X_test\n",
        "    res3=model3.predict(testing_data)\n",
        "    predictedClass = doFusion(res1,res2,res3,y_test,class_no=num_classes)\n",
        "    labels = np.argmax(y_test,axis=-1)\n",
        "    count=0\n",
        "    for i in range(len(labels)):\n",
        "        if predictedClass[i]==labels[i]:\n",
        "            count+=1\n",
        "    accuracy= (count/len(labels))*100\n",
        "    predictions=predictedClass\n",
        "    print(\"Accuracy of rank based fuzzy ensemble:\" + str(accuracy)+\"%\")\n",
        "    y_label=np.argmax(y_test,axis=1).tolist()\n",
        "    my_pred=[]\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i]==0:\n",
        "            my_pred.append('NORMAL')\n",
        "        elif predictions[i]==1:\n",
        "            my_pred.append('COVID-19')\n",
        "        elif predictions[i]==2:\n",
        "            my_pred.append('PNEUMONIA')\n",
        "    y_labels=[]\n",
        "    for i in range(len(y_label)):\n",
        "        if y_label[i]==0:\n",
        "            y_labels.append('NORMAL')\n",
        "        elif y_label[i]==1:\n",
        "            y_labels.append('COVID-19')\n",
        "        elif y_label[i]==2:\n",
        "            y_labels.append('PNEUMONIA')\n",
        "    cm=metrics.confusion_matrix(my_pred,y_labels)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                     index = ['NORMAL','COVID-19','PNEUMONIA'], \n",
        "                     columns = ['NORMAL','COVID-19','PNEUMONIA'])\n",
        "    \n",
        "#     plt.figure(figsize=(3,3))\n",
        "    sns.heatmap(cm_df, annot=True,fmt='g',annot_kws={'size':20})\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Actual Values')\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.savefig(\"Ensemble Rank Based Fuzzy.png\")\n",
        "    plt.show()\n",
        "    print(\"micro precision score of Rank Based Fuzzy:\",metrics.precision_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro precision score of Rank Based Fuzzy:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro recall score of Rank Based Fuzzy:\",metrics.recall_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro recall score of Rank Based Fuzzy:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro f1 score of Rank Based Fuzzy:\",f1_score(predictions, y_label, average='micro'))\n",
        "    print(\"macro f1 score of Rank Based Fuzzy:\",f1_score(predictions, y_label, average='macro'))\n",
        "    \n",
        "\n",
        "\n",
        "se_incpetion_v3(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label) #results of se inception v3 comes from this function\n",
        "densenet(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label) #results of densenet 201 comes from this function\n",
        "se_squeeznet(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label) #results of se squeezenet comes from this function\n",
        "rank_fuzzy(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label) #ensembled results comes from here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d0e152c",
      "metadata": {
        "id": "1d0e152c"
      },
      "source": [
        "### with segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34dec10e",
      "metadata": {
        "id": "34dec10e"
      },
      "outputs": [],
      "source": [
        "import os,glob\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "gpu=input(\"Which gpu number you would like to allocate:\")\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpu\n",
        "import glob\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import re\n",
        "import datetime\n",
        "import keras\n",
        "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
        "from tensorflow.keras.layers import concatenate,Flatten,Layer,ReLU, MaxPooling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.svm import LinearSVC\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skimage.feature import hog,local_binary_pattern\n",
        "from skimage import data, exposure\n",
        "from skimage.transform import radon, rescale\n",
        "from skimage.filters import roberts, sobel, scharr, prewitt\n",
        "from classification_models.keras import Classifiers\n",
        "from skimage import feature\n",
        "import os,glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import argparse\n",
        "import re\n",
        "import datetime\n",
        "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
        "from tensorflow.keras.layers import concatenate,Flatten,ConvLSTM2D,LayerNormalization,GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Sequential\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.svm import LinearSVC\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import loadtxt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skimage.feature import hog,local_binary_pattern\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "from skimage import data, exposure\n",
        "from tensorflow.keras.layers import Layer\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def no_data_augmentation(normal_files,covid_files,pneumonia_files):\n",
        "    aug_normal=[]\n",
        "    aug_covid=[]\n",
        "    aug_pneumonia=[]\n",
        "    for ele in normal_files:\n",
        "        #ele=ele/255.0\n",
        "        x = np.load(ele)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        aug_normal.append(x)\n",
        "    for ele in covid_files:\n",
        "        #ele=ele/255.0\n",
        "        x = np.load(ele)\n",
        "        \n",
        "        \n",
        "        \n",
        "        aug_covid.append(x)\n",
        "    for ele in pneumonia_files:\n",
        "        #ele=ele/255.0\n",
        "        x = np.load(ele)\n",
        "      \n",
        "      \n",
        "        aug_pneumonia.append(x)\n",
        "    \n",
        "    for i in range(len(aug_normal)):\n",
        "        aug_normal[i]=aug_normal[i].reshape((224,224))\n",
        "    \n",
        "    for i in range(len(aug_covid)):\n",
        "        aug_covid[i]=aug_covid[i].reshape((224,224))\n",
        "    for i in range(len(aug_pneumonia)):\n",
        "        aug_pneumonia[i]=aug_pneumonia[i].reshape((224,224))\n",
        "    print(\"Normal files without augmentation:\",len(aug_normal))\n",
        "    print(\"Covid files without augmentation:\", len(aug_covid))\n",
        "    print(\"Pneumonia files without augmentation:\",len(aug_pneumonia))\n",
        "    return aug_normal,aug_covid,aug_pneumonia\n",
        "\n",
        "def data_augmentation(normal_files,covid_files,pneumonia_files):\n",
        "    aug_normal=[]\n",
        "    aug_covid=[]\n",
        "    thresh_hold=7\n",
        "    aug_pneumonia=[]\n",
        "    \n",
        "    #x = tf.keras.preprocessing.image.load_img(\"/content/IM-0001-0001.jpeg\")\n",
        "    \n",
        "    datagen=ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "\n",
        "    )\n",
        "    #normal\n",
        "    counter=0\n",
        "    \n",
        "    for location in tqdm(normal_files):\n",
        "        counter=0\n",
        "\n",
        "        x = np.load(location)\n",
        "       \n",
        "        \n",
        "        x = np.expand_dims(x, axis=-1) \n",
        "        x=x.reshape((1,)+x.shape)\n",
        "        #x=x/255.0\n",
        "        \n",
        "\n",
        "        for i in datagen.flow(x):\n",
        "            if counter>=17:\n",
        "                break\n",
        "            #i=i/255.0\n",
        "\n",
        "            #i = cv2.resize(i,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "            aug_normal.append(i)\n",
        "            counter+=1\n",
        "    #covid\n",
        "    counter=0\n",
        "    for location in tqdm(covid_files):\n",
        "        counter=0\n",
        "        x = np.load(location)\n",
        "    \n",
        "        \n",
        "        x = np.expand_dims(x, axis=-1) \n",
        "        #x=img_to_array(x)\n",
        "        x=x.reshape((1,)+x.shape)\n",
        "        #x=x/255.0\n",
        "\n",
        "\n",
        "        for i in datagen.flow(x):\n",
        "            if counter>=2:\n",
        "                break\n",
        "\n",
        "            aug_covid.append(i)\n",
        "            counter+=1    \n",
        "    #pneumonia\n",
        "    counter=0\n",
        "    for location in tqdm(pneumonia_files):\n",
        "        counter=0\n",
        "        x = np.load(location)\n",
        "    \n",
        "\n",
        "      \n",
        "        x = np.expand_dims(x, axis=-1) \n",
        "        #x=img_to_array(x)\n",
        "        x=x.reshape((1,)+x.shape)\n",
        "        #x=x/255.0\n",
        "\n",
        "        for i in datagen.flow(x):\n",
        "            if counter>=3:\n",
        "                break\n",
        "            #i=i/255.0\n",
        "            #i = cv2.resize(i,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "            aug_pneumonia.append(i)\n",
        "            counter+=1    \n",
        "\n",
        "    for ele in normal_files:\n",
        "        #ele=ele/255.0\n",
        "        x = np.load(ele)\n",
        "        \n",
        "       \n",
        "        aug_normal.append(x)\n",
        "    for ele in covid_files:\n",
        "        #ele=ele/255.0\n",
        "        x = np.load(ele)\n",
        "        \n",
        "      \n",
        "        aug_covid.append(x)\n",
        "    for ele in pneumonia_files:\n",
        "        #ele=ele/255.0\n",
        "        x = np.load(ele)\n",
        "      \n",
        "      \n",
        "        aug_pneumonia.append(x)\n",
        "   \n",
        "    for i in range(len(aug_normal)):\n",
        "        aug_normal[i]=aug_normal[i].reshape((224,224))\n",
        "    \n",
        "    for i in range(len(aug_covid)):\n",
        "        aug_covid[i]=aug_covid[i].reshape((224,224))\n",
        "    for i in range(len(aug_pneumonia)):\n",
        "        aug_pneumonia[i]=aug_pneumonia[i].reshape((224,224))\n",
        "    print(\"Normal files after augmentation:\",np.shape(np.array(aug_normal)))\n",
        "    print(\"Covid files after augmentation:\", np.shape(np.array(aug_covid)))\n",
        "    print(\"Pneumonia files after augmentation:\",np.shape(np.array(aug_pneumonia)))\n",
        "    return aug_normal,aug_covid,aug_pneumonia\n",
        "\n",
        "def making_full_data(aug_normal,aug_covid,aug_pneumonia):\n",
        "    aug_normal=shuffle(aug_normal, random_state=0)\n",
        "    aug_covid=shuffle(aug_covid,random_state=0)\n",
        "    aug_pneumonia=shuffle(aug_pneumonia,random_state=0)\n",
        "    \n",
        "    aug_normal_labels=[]\n",
        "    for i in range(len(aug_normal)):\n",
        "        aug_normal_labels.append(0)\n",
        "    print(np.shape(aug_normal),np.shape(aug_normal_labels))\n",
        "    aug_covid_labels=[]\n",
        "    for i in range(len(aug_covid)):\n",
        "        aug_covid_labels.append(1)\n",
        "    print(np.shape(aug_covid),np.shape(aug_covid_labels))\n",
        "    aug_pneumonia_labels=[]\n",
        "    for i in range(len(aug_pneumonia)):\n",
        "        aug_pneumonia_labels.append(2)\n",
        "    print(np.shape(aug_pneumonia),np.shape(aug_pneumonia_labels))  \n",
        "\n",
        "    full_data=[]\n",
        "    full_label=[]\n",
        "    for i in range(len(aug_normal)):\n",
        "        full_data.append(aug_normal[i])\n",
        "        full_label.append(aug_normal_labels[i])\n",
        "    for i in range(len(aug_covid)):\n",
        "        full_data.append(aug_covid[i])\n",
        "        full_label.append(aug_covid_labels[i])\n",
        "    for i in range(len(aug_pneumonia)):\n",
        "        full_data.append(aug_pneumonia[i])\n",
        "        full_label.append(aug_pneumonia_labels[i])\n",
        "        \n",
        "    full_data=np.array(full_data)\n",
        "    full_label=np.array(full_label)\n",
        "    \n",
        "    full_data=shuffle(full_data,random_state=0)\n",
        "    full_label=shuffle(full_label,random_state=0)\n",
        "    \n",
        "    return full_data,full_label\n",
        "\"\"\"Inception 2D_CNN Models in Tensorflow-Keras.\n",
        "References -\n",
        "Inception_v1 (GoogLeNet): https://arxiv.org/abs/1409.4842 [Going Deeper with Convolutions]\n",
        "Inception_v2: http://arxiv.org/abs/1512.00567 [Rethinking the Inception Architecture for Computer Vision]\n",
        "Inception_v3: http://arxiv.org/abs/1512.00567 [Rethinking the Inception Architecture for Computer Vision]\n",
        "Inception_v4: https://arxiv.org/abs/1602.07261 [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Conv_2D_Block(x, model_width, kernel, strides=(1, 1), padding=\"same\"):\n",
        "    # 2D Convolutional Block with BatchNormalization\n",
        "    x = tf.keras.layers.Conv2D(model_width, kernel, strides=strides, padding=padding, kernel_initializer=\"he_normal\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def classifier(inputs, class_number):\n",
        "    # Construct the Classifier Group\n",
        "    # inputs       : input vector\n",
        "    # class_number : number of output classes\n",
        "    out = tf.keras.layers.Dense(class_number, activation='softmax')(inputs)\n",
        "    return out\n",
        "\n",
        "\n",
        "def regressor(inputs, feature_number):\n",
        "    # Construct the Regressor Group\n",
        "    # inputs         : input vector\n",
        "    # feature_number : number of output features\n",
        "    out = tf.keras.layers.Dense(feature_number, activation='linear')(inputs)\n",
        "    return out\n",
        "\n",
        "\n",
        "def SE_Block(inputs, num_filters, ratio):\n",
        "    squeeze = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n",
        "\n",
        "    excitation = tf.keras.layers.Dense(units=num_filters/ratio)(squeeze)\n",
        "    excitation = tf.keras.layers.Activation('relu')(excitation)\n",
        "    excitation = tf.keras.layers.Dense(units=num_filters)(excitation)\n",
        "    excitation = tf.keras.layers.Activation('sigmoid')(excitation)\n",
        "    excitation = tf.keras.layers.Reshape([1, 1, num_filters])(excitation)\n",
        "\n",
        "    scale = inputs * excitation\n",
        "\n",
        "    return scale\n",
        "\n",
        "\n",
        "def Inceptionv1_Module(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB4_1, i):\n",
        "    # Inception Block i\n",
        "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1), padding='valid')\n",
        "\n",
        "    branch3x3 = Conv_2D_Block(inputs, filterB2_1, (1, 1), padding='valid')\n",
        "    branch3x3 = Conv_2D_Block(branch3x3, filterB2_2, (3, 3))\n",
        "\n",
        "    branch5x5 = Conv_2D_Block(inputs, filterB3_1, (1, 1), padding='valid')\n",
        "    branch5x5 = Conv_2D_Block(branch5x5, filterB3_2, (5, 5))\n",
        "\n",
        "    branch_pool = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
        "    out = tf.keras.layers.concatenate([branch1x1, branch3x3, branch5x5, branch_pool], axis=-1, name='Inception_Block_'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def Inceptionv2_Module(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
        "    # Inception Block i\n",
        "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
        "\n",
        "    branch3x3 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
        "    branch3x3 = Conv_2D_Block(branch3x3, filterB2_2, (3, 3))\n",
        "\n",
        "    branch3x3dbl = Conv_2D_Block(inputs, filterB3_1, (1, 1))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (3, 3))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_3, (3, 3))\n",
        "\n",
        "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
        "\n",
        "    out = tf.keras.layers.concatenate([branch1x1, branch3x3, branch3x3dbl, branch_pool], axis=-1, name='Inception_Block_'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def Inception_Module_A(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
        "    # Inception Block i\n",
        "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
        "\n",
        "    branch5x5 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
        "    branch5x5 = Conv_2D_Block(branch5x5, filterB2_2, (5, 5))\n",
        "\n",
        "    branch3x3dbl = Conv_2D_Block(inputs, filterB3_1, (1, 1))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (3, 3))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_3, (3, 3))\n",
        "\n",
        "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
        "\n",
        "    out = tf.keras.layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=-1, name='Inception_Block_A'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def Inception_Module_B(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
        "    # Inception Block i\n",
        "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
        "\n",
        "    branch7x7 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
        "    branch7x7 = Conv_2D_Block(branch7x7, filterB2_2, (1, 7))\n",
        "    branch7x7 = Conv_2D_Block(branch7x7, filterB2_2, (7, 1))\n",
        "\n",
        "    branch7x7dbl = Conv_2D_Block(inputs, filterB3_1, 1)\n",
        "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_2, (1, 7))\n",
        "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_2, (7, 1))\n",
        "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_3, (1, 7))\n",
        "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_3, (7, 1))\n",
        "\n",
        "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
        "\n",
        "    out = tf.keras.layers.concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool], axis=-1, name='Inception_Block_B'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def Inception_Module_C(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
        "    # Inception Block i\n",
        "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
        "\n",
        "    branch3x3 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
        "    branch3x3_2 = Conv_2D_Block(branch3x3, filterB2_2, (1, 3))\n",
        "    branch3x3_3 = Conv_2D_Block(branch3x3, filterB2_2, (3, 1))\n",
        "\n",
        "    branch3x3dbl = Conv_2D_Block(inputs, filterB3_1, (1, 1))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (1, 3))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (3, 1))\n",
        "    branch3x3dbl_2 = Conv_2D_Block(branch3x3dbl, filterB3_3, (1, 3))\n",
        "    branch3x3dbl_3 = Conv_2D_Block(branch3x3dbl, filterB3_3, (3, 1))\n",
        "\n",
        "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
        "\n",
        "    out = tf.keras.layers.concatenate([branch1x1, branch3x3_2, branch3x3_3, branch3x3dbl_2, branch3x3dbl_3, branch_pool], axis=-1, name='Inception_Block_C'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def Reduction_Block_A(inputs, filterB1_1, filterB1_2, filterB2_1, filterB2_2, filterB2_3, i):\n",
        "    # Reduction Block A (i)\n",
        "    branch3x3 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
        "    branch3x3 = Conv_2D_Block(branch3x3, filterB1_2, (3, 3), strides=(2, 2))\n",
        "\n",
        "    branch3x3dbl = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_2, (3, 3))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_3, (3, 3), strides=(2, 2))\n",
        "\n",
        "    branch_pool = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(inputs)\n",
        "    out = tf.keras.layers.concatenate([branch3x3, branch3x3dbl, branch_pool], axis=-1, name='Reduction_Block_'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def Reduction_Block_B(inputs, filterB1_1, filterB1_2, filterB2_1, filterB2_2, filterB2_3, i):\n",
        "    # Reduction Block B (i)\n",
        "    branch3x3 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
        "    branch3x3 = Conv_2D_Block(branch3x3, filterB1_2, (3, 3), strides=(2, 2))\n",
        "\n",
        "    branch3x3dbl = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_2, (1, 7))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_2, (7, 1))\n",
        "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_3, (3, 3), strides=(2, 2))\n",
        "\n",
        "    branch_pool = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(inputs)\n",
        "    out = tf.keras.layers.concatenate([branch3x3, branch3x3dbl, branch_pool], axis=-1, name='Reduction_Block_'+str(i))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class SEInception:\n",
        "    def __init__(self, length, width, num_channel, num_filters, ratio=4, problem_type='Regression',\n",
        "                 output_nums=1, pooling='avg', dropout_rate=False, auxilliary_outputs=False):\n",
        "        # length: Input Signal Length\n",
        "        # model_depth: Depth of the Model\n",
        "        # model_width: Width of the Model\n",
        "        # kernel_size: Kernel or Filter Size of the Input Convolutional Layer\n",
        "        # num_channel: Number of Channels of the Input Predictor Signals\n",
        "        # problem_type: Regression or Classification\n",
        "        # output_nums: Number of Output Classes in Classification mode and output features in Regression mode\n",
        "        # pooling: Choose either 'max' for MaxPooling or 'avg' for Averagepooling\n",
        "        # dropout_rate: If turned on, some layers will be dropped out randomly based on the selected proportion\n",
        "        # auxilliary_outputs: Two extra Auxullary outputs for the Inception models, acting like Deep Supervision\n",
        "        self.length = length\n",
        "        self.width = width\n",
        "        self.num_channel = num_channel\n",
        "        self.num_filters = num_filters\n",
        "        self.ratio = ratio\n",
        "        self.problem_type = problem_type\n",
        "        self.output_nums = output_nums\n",
        "        self.pooling = pooling\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.auxilliary_outputs = auxilliary_outputs\n",
        "\n",
        "    def MLP(self, x):\n",
        "        if self.pooling == 'avg':\n",
        "            x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "        elif self.pooling == 'max':\n",
        "            x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
        "        if self.dropout_rate:\n",
        "            x = tf.keras.layers.Dropout(self.dropout_rate)(x)\n",
        "        # Final Dense Outputting Layer for the outputs\n",
        "        x = tf.keras.layers.Flatten()(x)\n",
        "        outputs = tf.keras.layers.Dense(self.output_nums, activation='linear')(x)\n",
        "        if self.problem_type == 'Classification':\n",
        "            outputs = tf.keras.layers.Dense(self.output_nums, activation='softmax')(x)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def SEInception_v1(self):\n",
        "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
        "        # Stem\n",
        "        x = Conv_2D_Block(inputs, self.num_filters, 7, strides=2)\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "        x = Conv_2D_Block(x, self.num_filters, 1, padding='valid')\n",
        "        x = Conv_2D_Block(x, self.num_filters * 3, 3)\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "\n",
        "        x = Inceptionv1_Module(x, 64, 96, 128, 16, 32, 32, 1)  # Inception Block 1\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv1_Module(x, 128, 128, 192, 32, 96, 64, 2)  # Inception Block 2\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_0 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 0\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
        "            aux_output_0 = self.MLP(aux_conv)\n",
        "\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "        x = Inceptionv1_Module(x, 192, 96, 208, 16, 48, 64, 3)  # Inception Block 3\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv1_Module(x, 160, 112, 224, 24, 64, 64, 4)  # Inception Block 4\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv1_Module(x, 128, 128, 256, 24, 64, 64, 5)  # Inception Block 5\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv1_Module(x, 112, 144, 288, 32, 64, 64, 6)  # Inception Block 6\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv1_Module(x, 256, 160, 320, 32, 128, 128, 7)  # Inception Block 7\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_1 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 1\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
        "            aux_output_1 = self.MLP(aux_conv)\n",
        "\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "        x = Inceptionv1_Module(x, 256, 160, 320, 32, 128, 128, 8)  # Inception Block 8\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv1_Module(x, 384, 192, 384, 48, 128, 128, 9)  # Inception Block 9\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        # Final Dense MLP Layer for the outputs\n",
        "        final_output = self.MLP(x)\n",
        "        # Create model.\n",
        "        model = tf.keras.Model(inputs, final_output, name='Inception_v3')\n",
        "        if self.auxilliary_outputs:\n",
        "            model = tf.keras.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v1')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def SEInception_v2(self):\n",
        "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
        "        # Stem: 56 x 64\n",
        "        x = tf.keras.layers.SeparableConv2D(self.num_filters, kernel_size=7, strides=(2, 2), depth_multiplier=1, padding='same')(inputs)\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "        x = Conv_2D_Block(x, self.num_filters * 2, 1, padding='valid')\n",
        "        x = Conv_2D_Block(x, self.num_filters * 6, 3, padding='valid')\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "\n",
        "        x = Inceptionv2_Module(x, 64, 64, 64, 64, 96, 96, 32, 1)  # Inception Block 1: 28 x 192\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv2_Module(x, 64, 64, 96, 64, 96, 96, 64, 2)  # Inception Block 2: 28 x 256\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_0 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 0\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
        "            aux_output_0 = self.MLP(aux_conv)\n",
        "\n",
        "        x = Reduction_Block_A(x, 128, 160, 64, 96, 96, 1)  # Reduction Block 1: 28 x 320\n",
        "\n",
        "        x = Inceptionv2_Module(x, 224, 64, 96, 96, 128, 128, 128, 3)  # Inception Block 3: 14 x 576\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv2_Module(x, 192, 96, 128, 96, 128, 128, 128, 4)  # Inception Block 4: 14 x 576\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv2_Module(x, 160, 128, 160, 128, 160, 160, 96, 5)  # Inception Block 5: 14 x 576\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv2_Module(x, 96, 128, 192, 160, 192, 192, 96, 6)  # Inception Block 6: 14 x 576\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_1 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 1\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 192, 1)\n",
        "            aux_output_1 = self.MLP(aux_conv)\n",
        "\n",
        "        x = Reduction_Block_A(x, 128, 192, 192, 256, 256, 2)  # Reduction Block 2: 14 x 576\n",
        "\n",
        "        x = Inceptionv2_Module(x, 352, 192, 320, 160, 224, 224, 128, 7)  # Inception Block 7: 7 x 1024\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inceptionv2_Module(x, 352, 192, 320, 192, 224, 224, 128, 8)  # Inception Block 8: 7 x 1024\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        # Final Dense MLP Layer for the outputs\n",
        "        final_output = self.MLP(x)\n",
        "        # Create model.\n",
        "        model = tf.keras.Model(inputs, final_output, name='Inception_v3')\n",
        "        if self.auxilliary_outputs:\n",
        "            model = tf.keras.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v2')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def SEInception_v3(self):\n",
        "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
        "        # Stem\n",
        "        x = Conv_2D_Block(inputs, self.num_filters, 3, strides=2, padding='valid')\n",
        "        x = Conv_2D_Block(x, self.num_filters, 3, padding='valid')\n",
        "        x = Conv_2D_Block(x, self.num_filters * 2, 3)\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "\n",
        "        x = Conv_2D_Block(x, self.num_filters * 2.5, 1, padding='valid')\n",
        "        x = Conv_2D_Block(x, self.num_filters * 6, 3, padding='valid')\n",
        "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "\n",
        "        # 3x Inception-A Blocks\n",
        "        x = Inception_Module_A(x, 64, 48, 64, 64, 96, 96, 32, 1)  # Inception-A Block 1: 35 x 256\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inception_Module_A(x, 64, 48, 64, 64, 96, 96, 64, 2)  # Inception-A Block 2: 35 x 256\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inception_Module_A(x, 64, 48, 64, 64, 96, 96, 64, 3)  # Inception-A Block 3: 35 x 256\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_0 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 0\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
        "            aux_output_0 = self.MLP(aux_conv)\n",
        "\n",
        "        x = Reduction_Block_A(x, 64, 384, 64, 96, 96, 1)  # Reduction Block 1: 17 x 768\n",
        "\n",
        "        # 4x Inception-B Blocks\n",
        "        x = Inception_Module_B(x, 192, 128, 192, 128, 128, 192, 192, 1)  # Inception-B Block 1: 17 x 768\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inception_Module_B(x, 192, 160, 192, 160, 160, 192, 192, 2)  # Inception-B Block 2: 17 x 768\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inception_Module_B(x, 192, 160, 192, 160, 160, 192, 192, 3)  # Inception-B Block 3: 17 x 768\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inception_Module_B(x, 192, 192, 192, 192, 192, 192, 192, 4)  # Inception-B Block 4: 17 x 768\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_1 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 1\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 192, 1)\n",
        "            aux_output_1 = self.MLP(aux_conv)\n",
        "\n",
        "        x = Reduction_Block_B(x, 192, 320, 192, 192, 192, 2)  # Reduction Block 2: 8 x 1280\n",
        "\n",
        "        # 2x Inception-C Blocks: 8 x 2048\n",
        "        x = Inception_Module_C(x, 320, 384, 384, 448, 384, 384, 192, 1)  # Inception-C Block 1: 8 x 2048\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "        x = Inception_Module_C(x, 320, 384, 384, 448, 384, 384, 192, 2)  # Inception-C Block 2: 8 x 2048\n",
        "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        # Final Dense MLP Layer for the outputs\n",
        "        final_output = self.MLP(x)\n",
        "        # Create model.\n",
        "        model = tf.keras.Model(inputs, final_output, name='Inception_v3')\n",
        "        if self.auxilliary_outputs:\n",
        "            model = tf.keras.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v3')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def SEInception_v4(self):\n",
        "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
        "        # Stem\n",
        "        x = Conv_2D_Block(inputs, 32, 3, strides=2, padding='valid')\n",
        "        x = Conv_2D_Block(x, 32, 3, padding='valid')\n",
        "        x = Conv_2D_Block(x, 64, 3)\n",
        "\n",
        "        branch1 = Conv_2D_Block(x, 96, 3, strides=2, padding='valid')\n",
        "        branch2 = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "        x = tf.keras.layers.concatenate([branch1, branch2], axis=-1)\n",
        "\n",
        "        branch1 = Conv_2D_Block(x, 64, 1)\n",
        "        branch1 = Conv_2D_Block(branch1, 96, 3, padding='valid')\n",
        "        branch2 = Conv_2D_Block(x, 64, 1)\n",
        "        branch2 = Conv_2D_Block(branch2, 64, 7)\n",
        "        branch2 = Conv_2D_Block(branch2, 96, 3, padding='valid')\n",
        "        x = tf.keras.layers.concatenate([branch1, branch2], axis=-1)\n",
        "\n",
        "        branch1 = Conv_2D_Block(x, 192, 3, padding='valid')\n",
        "        branch2 = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1))(x)\n",
        "        x = tf.keras.layers.concatenate([branch1, branch2], axis=-1)\n",
        "\n",
        "        # 4x Inception-A Blocks - 35 x 256\n",
        "        for i in range(4):\n",
        "            x = Inception_Module_A(x, 96, 64, 96, 64, 96, 96, 96, i)\n",
        "            x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_0 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 0\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 96, 1)\n",
        "            aux_output_0 = self.MLP(aux_conv)\n",
        "\n",
        "        x = Reduction_Block_A(x, 64, 384, 192, 224, 256, 1)  # Reduction Block 1: 17 x 768\n",
        "\n",
        "        # 7x Inception-B Blocks - 17 x 768\n",
        "        for i in range(7):\n",
        "            x = Inception_Module_B(x, 384, 192, 256, 192, 224, 256, 128, i)\n",
        "            x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        aux_output_1 = []\n",
        "        if self.auxilliary_outputs:\n",
        "            # Auxilliary Output 1\n",
        "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
        "            aux_conv = Conv_2D_Block(aux_pool, 128, 1)\n",
        "            aux_output_1 = self.MLP(aux_conv)\n",
        "\n",
        "        x = Reduction_Block_B(x, 192, 192, 256, 320, 320, 2)  # Reduction Block 2: 8 x 1280\n",
        "\n",
        "        # 3x Inception-C Blocks: 8 x 2048\n",
        "        for i in range(3):\n",
        "            x = Inception_Module_C(x, 256, 384, 512, 384, 512, 512, 256, i)\n",
        "            x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
        "\n",
        "        # Final Dense MLP Layer for the outputs\n",
        "        final_output = self.MLP(x)\n",
        "        # Create model.\n",
        "        model = tf.keras.Model(inputs, final_output, name='Inception_v4')\n",
        "        if self.auxilliary_outputs:\n",
        "            model = tf.keras.layers.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v4')\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "def making_training_and_testing_data(full_data,full_label):\n",
        "    \n",
        "    \n",
        "    train_label=[]\n",
        "    for i in range(len(full_label)):\n",
        "        if full_label[i]==0:\n",
        "            train_label.append([0,1,0])\n",
        "        elif full_label[i]==1:\n",
        "            train_label.append([1,0,0])\n",
        "        elif full_label[i]==2:\n",
        "\n",
        "            train_label.append([0,0,1])\n",
        "\n",
        "    \n",
        "    full_label=np.array(train_label)\n",
        "    \n",
        "    \n",
        "    return full_data,full_label\n",
        "    \n",
        "def my_plots(history,my_model):\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    my_path=\"training and validation accuracy curve of \"+my_model+\".png\"\n",
        "    plt.savefig(my_path)\n",
        "    plt.show()\n",
        "    \n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.ylim([0, 1])\n",
        "\n",
        "    #plt.ylim([-3, 3])\n",
        "    plt.yticks(np.arange(0, 1.1, 0.25))\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    my_path=\"training and validation loss curve of \"+my_model+\".png\"\n",
        "    plt.savefig(my_path)\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "    \n",
        "def SE_Block(inputs, num_filters, ratio):\n",
        "    squeeze = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n",
        "\n",
        "    excitation = tf.keras.layers.Dense(units=num_filters/ratio)(squeeze)\n",
        "    excitation = tf.keras.layers.Activation('relu')(excitation)\n",
        "    excitation = tf.keras.layers.Dense(units=num_filters)(excitation)\n",
        "    excitation = tf.keras.layers.Activation('sigmoid')(excitation)\n",
        "    excitation = tf.keras.layers.Reshape([1, 1, num_filters])(excitation)\n",
        "\n",
        "    scale = inputs * excitation\n",
        "    return scale\n",
        "\n",
        "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
        "    s_id = 'fire' + str(fire_id) + '/'\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = 3\n",
        "    \n",
        "    x = Conv2D(squeeze, (1, 1), padding='valid')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    left = Conv2D(expand, (1, 1), padding='valid')(x)\n",
        "    left = Activation('relu')(left)\n",
        "\n",
        "    right = Conv2D(expand, (3, 3), padding='same')(x)\n",
        "    right = Activation('relu')(right)\n",
        "\n",
        "    x = concatenate([left, right], axis=channel_axis)\n",
        "    return x\n",
        "from keras.utils import get_file\n",
        "def SE_SQUEEZNET(inputs,ratio,num_of_class):\n",
        "    x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(inputs)\n",
        "    x = Activation('relu', name='relu_conv1')(x)\n",
        "    x = MaxPool2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    \n",
        "    \n",
        "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool2')(x)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    \n",
        "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    \n",
        "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    \n",
        "    \n",
        "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool4')(x)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)    \n",
        "    \n",
        "\n",
        "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)  \n",
        "\n",
        "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
        "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
        "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    output = Dense(num_of_class, activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs, [output])\n",
        "    \n",
        "    \n",
        "    return model\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    normal_dir = \"\" #give your normal cases data path here\n",
        "    #vit_datasets/Dataset_ViT/ViT_dataset/Covid-19\n",
        "    dir1 = os.path.join(normal_dir,\"*.npy\")\n",
        "    dir2 = os.path.join(normal_dir,\"*.jpeg\")\n",
        "    dir = os.path.join(normal_dir,\"*.jpg\")\n",
        "    normal_files = glob.glob(dir)\n",
        "    normal_1 = glob.glob(dir1)\n",
        "    normal_2 = glob.glob(dir2)\n",
        "    normal_files.extend(normal_1)\n",
        "    normal_files.extend(normal_2)\n",
        "\n",
        "    normal_dir = \"\"  #give your covid 19 cases data path here\n",
        "    dir1 = os.path.join(normal_dir,\"*.npy\")\n",
        "    dir = os.path.join(normal_dir,\"*.jpg\")\n",
        "    dir2 = os.path.join(normal_dir,\"*.jpeg\")\n",
        "    covid_files = glob.glob(dir)\n",
        "    covid_files2 = glob.glob(dir2)\n",
        "    covid_files1 = glob.glob(dir1)\n",
        "    covid_files.extend(covid_files2)\n",
        "    covid_files.extend(covid_files1)\n",
        "\n",
        "    normal_dir = \"\" #give your pneumonia cases data path here\n",
        "    dir1 = os.path.join(normal_dir,\"*.npy\")\n",
        "    dir2 = os.path.join(normal_dir,\"*.jpeg\")\n",
        "    dir = os.path.join(normal_dir,\"*.jpg\")\n",
        "    pneumonia_files = glob.glob(dir)\n",
        "    pneumonia_1 = glob.glob(dir1)\n",
        "    pneumonia_2 = glob.glob(dir2)\n",
        "    pneumonia_files.extend(pneumonia_1)\n",
        "    pneumonia_files.extend(pneumonia_2)\n",
        "\n",
        "    normal_files.sort()\n",
        "    covid_files.sort()\n",
        "    pneumonia_files.sort()\n",
        "    normal_files=shuffle(normal_files,random_state=10)\n",
        "    covid_files=shuffle(covid_files,random_state=10)\n",
        "    pneumonia_files=shuffle(pneumonia_files,random_state=10)\n",
        "    print(\"pneumonia_files:\",len(pneumonia_files))\n",
        "    print(\"covid_files:\",len(covid_files))\n",
        "    print(\"normal_files:\",len(normal_files))\n",
        "    x=(len(normal_files)+len(covid_files)+len(pneumonia_files))*0.20\n",
        "    y=(len(normal_files)+len(covid_files)+len(pneumonia_files))*0.10\n",
        "    x=int(x//3)\n",
        "    y=int(y//3)\n",
        "    print(x)\n",
        "    print(y)\n",
        "    \n",
        "    for_normal=x-200\n",
        "    for_covid=x+150\n",
        "    for_pneumonia=x+50\n",
        "    test_normal_files=normal_files[:for_normal]\n",
        "    test_covid_files=covid_files[:for_covid]\n",
        "    test_pneumonia_files=pneumonia_files[:for_pneumonia]\n",
        "    \n",
        "    val_normal_files=normal_files[for_normal:for_normal+y]\n",
        "    val_covid_files=covid_files[for_covid:for_covid+y]\n",
        "    val_pneumonia_files=pneumonia_files[for_pneumonia:for_pneumonia+y]\n",
        "    \n",
        "    train_normal_files=normal_files[for_normal+y:]\n",
        "    train_covid_files=covid_files[for_covid+y:]\n",
        "    train_pneumonia_files=pneumonia_files[for_pneumonia+y:]\n",
        "    print(\"test normal:\",len(test_normal_files))\n",
        "    print(\"test covid:\",len(test_covid_files))\n",
        "    print(\"test pneumonia:\",len(test_pneumonia_files))\n",
        "    print(\"val normal:\",len(val_normal_files))\n",
        "    print(\"val covid:\",len(val_covid_files))\n",
        "    print(\"val pneumonia:\",len(val_pneumonia_files))\n",
        "    print(\"train normal:\",len(train_normal_files))\n",
        "    print(\"train covid:\",len(train_covid_files))\n",
        "    print(\"train pneumonia:\",len(train_pneumonia_files))\n",
        "    \n",
        "    \n",
        "    \n",
        "    train_aug_normal,train_aug_covid,train_aug_pneumonia=data_augmentation(train_normal_files,train_covid_files,train_pneumonia_files)\n",
        "    test_aug_normal,test_aug_covid,test_aug_pneumonia=no_data_augmentation(test_normal_files,test_covid_files,test_pneumonia_files)\n",
        "    val_aug_normal,val_aug_covid,val_aug_pneumonia=no_data_augmentation(val_normal_files,val_covid_files,val_pneumonia_files)\n",
        "    \n",
        "    train_full_data,train_full_label=making_full_data(train_aug_normal,train_aug_covid,train_aug_pneumonia)  #getting my full data\n",
        "    test_full_data,test_full_label=making_full_data(test_aug_normal,test_aug_covid,test_aug_pneumonia)\n",
        "    val_full_data,val_full_label=making_full_data(val_aug_normal,val_aug_covid,val_aug_pneumonia)\n",
        "    \n",
        "    train_full_data,train_full_label= making_training_and_testing_data(train_full_data,train_full_label) #dividing full_data into train and test data\n",
        "    test_full_data,test_full_label=making_training_and_testing_data(test_full_data,test_full_label)\n",
        "    val_full_data,val_full_label=making_training_and_testing_data(val_full_data,val_full_label)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db409214",
      "metadata": {
        "id": "db409214"
      },
      "outputs": [],
      "source": [
        "## from keras.utils import get_file\n",
        "import os,glob\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from classification_models.keras import Classifiers\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import pickle\n",
        "#import clahe\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import re\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sympy.solvers import solve\n",
        "from sympy import Symbol\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
        "from tensorflow.keras.layers import concatenate,Flatten,Layer,ReLU, MaxPooling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.svm import LinearSVC\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "from sklearn.metrics import f1_score\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skimage.feature import hog,local_binary_pattern\n",
        "from skimage import data, exposure\n",
        "from skimage.transform import radon, rescale\n",
        "from skimage.filters import roberts, sobel, scharr, prewitt\n",
        "from skimage import feature\n",
        "import os,glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import argparse\n",
        "import re\n",
        "import datetime\n",
        "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
        "from tensorflow.keras.layers import concatenate,Flatten,ConvLSTM2D,LayerNormalization,GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Sequential\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.svm import LinearSVC\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import loadtxt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skimage.feature import hog,local_binary_pattern\n",
        "from skimage import data, exposure\n",
        "from tensorflow.keras.layers import Layer\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "def se_incpetion_v3(X_train,X_test,X_val,y_train,y_test,y_val):\n",
        "    #pickled_model = pickle.load(open('SE_INCEPTION_V3.sav', 'rb'))\n",
        "    pickled_model = pickle.load(open('/home/pranab_2021cs25/S_Sharma/CIKM_Fuzzy_CXR_FL/SE_INCEPTION_V3_MODEL_with_segmentation.sav', 'rb'))\n",
        "    testing_data=X_test\n",
        "    pred=pickled_model.predict(testing_data)\n",
        "    predictions = np.argmax(pred,axis = 1)\n",
        "    y_label=np.argmax(y_test,axis = 1)\n",
        "    print(\"Accuracy of SE Inception V3 is:\",accuracy_score(predictions,y_label))\n",
        "    my_pred=[]\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i]==0:\n",
        "            my_pred.append('NORMAL')\n",
        "        elif predictions[i]==1:\n",
        "            my_pred.append('COVID-19')\n",
        "        elif predictions[i]==2:\n",
        "            my_pred.append('PNEUMONIA')\n",
        "    y_labels=[]\n",
        "    for i in range(len(y_label)):\n",
        "        if y_label[i]==0:\n",
        "            y_labels.append('NORMAL')\n",
        "        elif y_label[i]==1:\n",
        "            y_labels.append('COVID-19')\n",
        "        elif y_label[i]==2:\n",
        "            y_labels.append('PNEUMONIA')\n",
        "    \n",
        "    cm=metrics.confusion_matrix(my_pred,y_labels)\n",
        "    \n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index = ['NORMAL','COVID-19','PNEUMONIA'], \n",
        "                         columns = ['NORMAL','COVID-19','PNEUMONIA'])\n",
        "    \n",
        "#     plt.figure(figsize=(10,10))\n",
        "    sns.heatmap(cm_df, annot=True,fmt='g',annot_kws={'size':20})\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Actual Values')\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.savefig(\"SE Inception V3 Confusion matrix with segmentation.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"micro precision score of SE Inception V3:\",metrics.precision_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro precision score of SE Inception V3:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro recall score of SE Inception V3:\",metrics.recall_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro recall score of SE Inception V3:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro f1 score of SE Inception V3:\",f1_score(predictions, y_label, average='micro'))\n",
        "    print(\"macro f1 score of SE Inception V3:\",f1_score(predictions, y_label, average='macro'))\n",
        "    print(\"roc auc score of SE Inception V3\",roc_auc_score(y_test, pred,multi_class='ovr'))\n",
        "    #covid=[1,0,0]\n",
        "    #normal=[0,1,0]\n",
        "    #pneumonia=[0,0,1]\n",
        "    y_test_binarized=y_test\n",
        "\n",
        "\n",
        "# roc curve for classes\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    thresh ={}\n",
        "    roc_auc = dict()\n",
        "    classes=['COVID-19','NORMAL','PNEUMONIA']\n",
        "    n_class = 3\n",
        "\n",
        "    for i in range(n_class):    \n",
        "        fpr[i], tpr[i], thresh[i] = roc_curve(y_test_binarized[:,i], pred[:,i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # plotting    \n",
        "        plt.plot(fpr[i], tpr[i], linestyle='--', \n",
        "                 label='%s vs Rest (AUC=%0.2f)'%(classes[i],roc_auc[i]))\n",
        "\n",
        "    plt.plot([0,1],[0,1],'b--')\n",
        "    plt.xlim([0,1])\n",
        "    plt.ylim([0,1.05])\n",
        "    plt.title('Multiclass ROC curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.savefig(\"AUC ROC CURVE SE Inception V3 with segmentation.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    \n",
        "import keras\n",
        "from classification_models.keras import Classifiers\n",
        "def densenet(X_train,X_test,X_val,y_train,y_test,y_val):\n",
        "    #pickled_model = pickle.load(open('SE_RESNEXT_101.sav', 'rb'))\n",
        "    pickled_model = pickle.load(open('/home/pranab_2021cs25/S_Sharma/CIKM_Fuzzy_CXR_FL/densent_201_model_with_segmentation.sav', 'rb'))\n",
        "    testing_data=np.stack((X_test,)*3,axis=-1)\n",
        "    \n",
        "    pred=pickled_model.predict(testing_data)\n",
        "    predictions = np.argmax(pred,axis = 1)\n",
        "    y_label=np.argmax(y_test,axis = 1)\n",
        "    print(\"Accuracy of DENSENET 201 is:\",accuracy_score(predictions,y_label))\n",
        "    my_pred=[]\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i]==0:\n",
        "            my_pred.append('NORMAL')\n",
        "        elif predictions[i]==1:\n",
        "            my_pred.append('COVID-19')\n",
        "        elif predictions[i]==2:\n",
        "            my_pred.append('PNEUMONIA')\n",
        "    y_labels=[]\n",
        "    for i in range(len(y_label)):\n",
        "        if y_label[i]==0:\n",
        "            y_labels.append('NORMAL')\n",
        "        elif y_label[i]==1:\n",
        "            y_labels.append('COVID-19')\n",
        "        elif y_label[i]==2:\n",
        "            y_labels.append('PNEUMONIA')\n",
        "    cm=metrics.confusion_matrix(my_pred,y_labels)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                     index = ['NORMAL','COVID-19','PNEUMONIA'], \n",
        "                     columns = ['NORMAL','COVID-19','PNEUMONIA'])\n",
        "#     plt.figure(figsize=(10,10))\n",
        "    sns.heatmap(cm_df, annot=True,fmt='g',annot_kws={'size':20})\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Actual Values')\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.savefig(\"DENSENET 201 Confusion matrix with segmentation.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"micro precision score of DENSENET 201:\",metrics.precision_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro precision score of DENSENET 201:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro recall score of DENSENET 201:\",metrics.recall_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro recall score of DENSENET 201:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro f1 score of DENSENET 201:\",f1_score(predictions, y_label, average='micro'))\n",
        "    print(\"macro f1 score of DENSENET 201:\",f1_score(predictions, y_label, average='macro'))\n",
        "    print(\"roc auc score of DENSENET 201:\",roc_auc_score(y_test, pred,multi_class='ovr'))\n",
        "    \n",
        "    y_test_binarized=y_test\n",
        "\n",
        "\n",
        "    # roc curve for classes\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    thresh ={}\n",
        "    roc_auc = dict()\n",
        "    classes=['COVID-19','NORMAL','PNEUMONIA']\n",
        "    n_class = 3\n",
        "\n",
        "    for i in range(n_class):    \n",
        "        fpr[i], tpr[i], thresh[i] = roc_curve(y_test_binarized[:,i], pred[:,i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # plotting    \n",
        "        plt.plot(fpr[i], tpr[i], linestyle='--', \n",
        "                 label='%s vs Rest (AUC=%0.2f)'%(classes[i],roc_auc[i]))\n",
        "\n",
        "    plt.plot([0,1],[0,1],'b--')\n",
        "    plt.xlim([0,1])\n",
        "    plt.ylim([0,1.05])\n",
        "    plt.title('Multiclass ROC curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.savefig(\"AUC ROC CURVE DENSENET 201 with segmentation.png\")\n",
        "    plt.show()\n",
        "\n",
        "def se_squeeznet(X_train,X_test,X_val,y_train,y_test,y_val):\n",
        "    #pickled_model = pickle.load(open('normal_densent_201.sav', 'rb'))\n",
        "       \n",
        "    pickled_model = pickle.load(open('/home/pranab_2021cs25/S_Sharma/CIKM_Fuzzy_CXR_FL/se_squeeznet_MODEL_with_segmentation.sav', 'rb'))\n",
        "    \n",
        "    pred=pickled_model.predict(X_test)\n",
        "    predictions = np.argmax(pred,axis = 1)\n",
        "    y_label=np.argmax(y_test,axis = 1)\n",
        "    print(\"Accuracy of se_squeeznet is:\",accuracy_score(predictions,y_label))\n",
        "    my_pred=[]\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i]==0:\n",
        "            my_pred.append('NORMAL')\n",
        "        elif predictions[i]==1:\n",
        "            my_pred.append('COVID-19')\n",
        "        elif predictions[i]==2:\n",
        "            my_pred.append('PNEUMONIA')\n",
        "    y_labels=[]\n",
        "    for i in range(len(y_label)):\n",
        "        if y_label[i]==0:\n",
        "            y_labels.append('NORMAL')\n",
        "        elif y_label[i]==1:\n",
        "            y_labels.append('COVID-19')\n",
        "        elif y_label[i]==2:\n",
        "            y_labels.append('PNEUMONIA')\n",
        "    \n",
        "    cm=metrics.confusion_matrix(my_pred,y_labels)\n",
        "    \n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index =['NORMAL','COVID-19','PNEUMONIA'], \n",
        "                         columns = ['NORMAL','COVID-19','PNEUMONIA'])\n",
        "    \n",
        "#     plt.figure(figsize=(10,10))\n",
        "    sns.heatmap(cm_df, annot=True,fmt='g',annot_kws={'size':20})\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Actual Values')\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.savefig(\"se_squeeznet Confusion matrix with segmentation.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"micro precision score of se_squeeznet:\",metrics.precision_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro precision score of se_squeeznet:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro recall score of se_squeeznet:\",metrics.recall_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro recall score of se_squeeznet:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro f1 score of se_squeeznet:\",f1_score(predictions, y_label, average='micro'))\n",
        "    print(\"macro f1 score of se_squeeznet:\",f1_score(predictions, y_label, average='macro'))\n",
        "    print(\"roc auc score of se_squeeznet:\",roc_auc_score(y_test, pred,multi_class='ovr'))\n",
        "    #covid=[1,0,0]\n",
        "    #normal=[0,1,0]\n",
        "    #pneumonia=[0,0,1]\n",
        "    y_test_binarized=y_test\n",
        "\n",
        "\n",
        "# roc curve for classes\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    thresh ={}\n",
        "    roc_auc = dict()\n",
        "    classes=['COVID-19','NORMAL','PNEUMONIA']\n",
        "    n_class = 3\n",
        "\n",
        "    for i in range(n_class):    \n",
        "        fpr[i], tpr[i], thresh[i] = roc_curve(y_test_binarized[:,i], pred[:,i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # plotting    \n",
        "        plt.plot(fpr[i], tpr[i], linestyle='--', \n",
        "                 label='%s vs Rest (AUC=%0.2f)'%(classes[i],roc_auc[i]))\n",
        "\n",
        "    plt.plot([0,1],[0,1],'b--')\n",
        "    plt.xlim([0,1])\n",
        "    plt.ylim([0,1.05])\n",
        "    plt.title('Multiclass ROC curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.savefig(\"AUC ROC CURVE se_squeeznet with segmentation.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#Fuzzy Rank-based Ensemble:\n",
        "def getScore(model,test_imgs):\n",
        "  res = model.predict(test_imgs)\n",
        "  return res \n",
        "\n",
        "def generateRank1(score,class_no):\n",
        "  rank = np.zeros([class_no,1])\n",
        "  scores = np.zeros([class_no,1])\n",
        "  scores = score\n",
        "  for i in range(class_no):\n",
        "      rank[i] = 1 - np.exp(-((scores[i]-1)**2)/2.0)\n",
        "  return rank\n",
        "\n",
        "def generateRank2(score,class_no):\n",
        "  rank = np.zeros([class_no,1])\n",
        "  scores = np.zeros([class_no,1])\n",
        "  scores = score\n",
        "  for i in range(class_no):\n",
        "      rank[i] = 1 - np.tanh(((scores[i]-1)**2)/2)\n",
        "  return rank\n",
        "#[1,2,3]+[2,3,4]=[3,5,7] [0,1,0]  [1,0,0]  [0,0,1]\n",
        "def doFusion(res1,res2,res3,label,class_no):\n",
        "  cnt = 0\n",
        "  id = []\n",
        "  for i in range(len(res1)):\n",
        "      rank1 = generateRank1(res1[i],class_no)*generateRank2(res1[i],class_no)\n",
        "      rank2 = generateRank1(res2[i],class_no)*generateRank2(res2[i],class_no)\n",
        "      rank3 = generateRank1(res3[i],class_no)*generateRank2(res3[i],class_no)\n",
        "      rankSum = rank1 + rank2 + rank3 #list\n",
        "      rankSum = np.array(rankSum)\n",
        "#       scoreSum = 1 - (res1[i] + res2[i] + res3[i])/3\n",
        "#       scoreSum = np.array(scoreSum)\n",
        "      \n",
        "#       fusedScore = (rankSum.T)*scoreSum\n",
        "      cls = np.argmin(rankSum)\n",
        "      if cls<class_no and label[i][cls]== 1:\n",
        "          cnt += 1\n",
        "      id.append(cls)\n",
        "  print(cnt/len(res1))\n",
        "  return id   \n",
        "    \n",
        "def rank_fuzzy(X_train,X_test,X_val,y_train,y_test,y_val):\n",
        "    model1 = pickle.load(open('/home/pranab_2021cs25/S_Sharma/CIKM_Fuzzy_CXR_FL/SE_INCEPTION_V3_MODEL_with_segmentation.sav', 'rb'))\n",
        "    model2 = pickle.load(open('/home/pranab_2021cs25/S_Sharma/CIKM_Fuzzy_CXR_FL/densent_201_model_with_segmentation.sav', 'rb'))\n",
        "    model3 = pickle.load(open('/home/pranab_2021cs25/S_Sharma/CIKM_Fuzzy_CXR_FL/se_squeeznet_MODEL_with_segmentation.sav', 'rb'))\n",
        "    testing_data=X_test\n",
        "    res1 = model1.predict(testing_data)\n",
        "    num_classes=3\n",
        "    testing_data=np.stack((X_test,)*3,axis=-1)\n",
        "    \n",
        "    res2 = model2.predict(testing_data)\n",
        "    testing_data=X_test\n",
        "    res3=model3.predict(testing_data)\n",
        "    predictedClass = doFusion(res1,res2,res3,y_test,class_no=num_classes)\n",
        "    labels = np.argmax(y_test,axis=-1)\n",
        "    count=0\n",
        "    for i in range(len(labels)):\n",
        "        if predictedClass[i]==labels[i]:\n",
        "            count+=1\n",
        "    accuracy= (count/len(labels))*100\n",
        "    predictions=predictedClass\n",
        "    print(\"Accuracy of rank based fuzzy ensemble:\" + str(accuracy)+\"%\")\n",
        "    y_label=np.argmax(y_test,axis=1).tolist()\n",
        "    my_pred=[]\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i]==0:\n",
        "            my_pred.append('NORMAL')\n",
        "        elif predictions[i]==1:\n",
        "            my_pred.append('COVID-19')\n",
        "        elif predictions[i]==2:\n",
        "            my_pred.append('PNEUMONIA')\n",
        "    y_labels=[]\n",
        "    for i in range(len(y_label)):\n",
        "        if y_label[i]==0:\n",
        "            y_labels.append('NORMAL')\n",
        "        elif y_label[i]==1:\n",
        "            y_labels.append('COVID-19')\n",
        "        elif y_label[i]==2:\n",
        "            y_labels.append('PNEUMONIA')\n",
        "    cm=metrics.confusion_matrix(my_pred,y_labels)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                     index = ['NORMAL','COVID-19','PNEUMONIA'], \n",
        "                     columns = ['NORMAL','COVID-19','PNEUMONIA'])\n",
        "    \n",
        "#     plt.figure(figsize=(10,10))\n",
        "    sns.heatmap(cm_df, annot=True,fmt='g',annot_kws={'size':20})\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Actual Values')\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.savefig(\"Ensemble Rank Based Fuzzy with segmentation.png\")\n",
        "    plt.show()\n",
        "    print(\"micro precision score of Rank Based Fuzzy:\",metrics.precision_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro precision score of Rank Based Fuzzy:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro recall score of Rank Based Fuzzy:\",metrics.recall_score(predictions,y_label,average='micro'))\n",
        "    print(\"macro recall score of Rank Based Fuzzy:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
        "    print(\"micro f1 score of Rank Based Fuzzy:\",f1_score(predictions, y_label, average='micro'))\n",
        "    print(\"macro f1 score of Rank Based Fuzzy:\",f1_score(predictions, y_label, average='macro'))\n",
        "    \n",
        "se_incpetion_v3(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label) #results of se inception v3 comes from this function\n",
        "densenet(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label) #results of densenet 201 comes from this function\n",
        "se_squeeznet(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label) #results of se squeezenet comes from this function\n",
        "rank_fuzzy(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label) #results of ensemble comes from here\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}