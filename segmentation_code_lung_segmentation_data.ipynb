{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e80f1f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which gpu u want?4\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/COVID-19/images/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_251820/2454116734.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mmask_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/COVID-19/lung masks/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mmy_train_samples_covid_19\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mmy_train_samples_non_covid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/Non-COVID/images/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mmy_train_samples_normal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/Normal/images/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/COVID-19/images/'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#import tensorflow as tf\n",
    "from datetime import datetime \n",
    "from keras_unet_collection import models, losses\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "import os\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import segmentation_models as sm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import os \n",
    "import sys \n",
    "import random \n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np \n",
    "from time import time \n",
    "import segmentation_models as sm\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Activation, add, multiply, UpSampling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K \n",
    "smooth = 1\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n",
    "from tensorflow.keras import backend as K \n",
    "smooth = 1\n",
    "\n",
    "\n",
    "def iou_score(y_pred, y_true, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    union = K.sum(y_true, -1) + K.sum(y_pred, -1) - intersection\n",
    "    iou = (intersection + smooth)/(union + smooth)\n",
    "    return iou\n",
    "if __name__ == '__main__':\n",
    "    gpu=int(input(\"Which gpu u want?\"))\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu)\n",
    "    image_directory = '/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/COVID-19/images/'\n",
    "    mask_directory = '/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/COVID-19/lung masks/'\n",
    "    images=[]\n",
    "    my_train_samples_covid_19=os.listdir(image_directory)\n",
    "    my_train_samples_non_covid=os.listdir(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/Non-COVID/images/\")\n",
    "    my_train_samples_normal=os.listdir(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/Normal/images/\")\n",
    "    for i in my_train_samples_covid_19:\n",
    "        images.append(image_directory+i)\n",
    "    for i in my_train_samples_non_covid:\n",
    "        images.append(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/Non-COVID/images/\"+i)\n",
    "    for i in my_train_samples_normal:\n",
    "        images.append(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/Normal/images/\"+i)\n",
    "    print(\"appending masks\")\n",
    "    masks=[]\n",
    "    my_train_samples_covid_19=os.listdir(mask_directory)\n",
    "    my_train_samples_non_covid=os.listdir(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/Non-COVID/lung masks/\")\n",
    "    my_train_samples_normal=os.listdir(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/Normal/lung masks/\")\n",
    "    for i in my_train_samples_covid_19:\n",
    "        masks.append(mask_directory+i)\n",
    "    for i in my_train_samples_non_covid:\n",
    "        masks.append(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/Non-COVID/lung masks/\"+i)\n",
    "    for i in my_train_samples_normal:\n",
    "        masks.append(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Train/Normal/lung masks/\"+i)\n",
    "    print(\"shuffling images and masks\")\n",
    "    images=shuffle(images, random_state=0)\n",
    "    masks=shuffle(masks,random_state=0)\n",
    "    for i in images:\n",
    "        x=cv2.imread(i)\n",
    "        i=np.array(x)\n",
    "        if i.shape!=(256,256,3):\n",
    "            print(i.shape)\n",
    "    for i in masks:\n",
    "        x=cv2.imread(i)\n",
    "        i=np.array(x)\n",
    "        if i.shape!=(256,256,3):\n",
    "            print(i.shape)\n",
    "            \n",
    "    print(\"creating image and mask dataset\")\n",
    "    image_dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
    "    mask_dataset = []  #Place holders to define add labels. We will add 0 to all parasitized images and 1 to uninfected.\n",
    "\n",
    "\n",
    "    for i, image_name in enumerate(images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
    "        #if (image_name.split('.')[1] == 'tif'):\n",
    "            #print(image_directory+image_name)\n",
    "\n",
    "        img = Image.open(image_name).convert('L')\n",
    "        image=np.array(img)\n",
    "        #image = image.resize(image,(SIZE, SIZE))[:,:,0]\n",
    "        image_dataset.append(image)\n",
    "\n",
    "    #Iterate through all images in Uninfected folder, resize to 64 x 64\n",
    "    #Then save into the same numpy array 'dataset' but with label 1\n",
    "\n",
    "\n",
    "    for i, image_name in enumerate(masks):\n",
    "        img = Image.open(image_name).convert('L')\n",
    "        image=np.array(img)\n",
    "        #image = image.resize(image,(SIZE, SIZE))[:,:,0]\n",
    "        mask_dataset.append(image)\n",
    "\n",
    "    #Normalize images\n",
    "    print(\"doing expand dims\")\n",
    "    #mage_dataset=np.stack((image_dataset,)*3,axis=-1)\n",
    "    image_dataset = np.array(image_dataset)/255.\n",
    "    train_image_dataset=np.expand_dims((np.array(image_dataset)),3)\n",
    "    #D not normalize masks, just rescale to 0 to 1.\n",
    "    #mask_dataset=np.stack((mask_dataset,)*3,axis=-1)\n",
    "    train_mask_dataset = np.expand_dims((np.array(mask_dataset)),3) /255.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #now test data\n",
    "    image_directory = '/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Test/COVID-19/images/'\n",
    "    mask_directory = '/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Test/COVID-19/lung masks/'\n",
    "    images=[]\n",
    "    my_test_samples_covid_19=os.listdir(image_directory)\n",
    "    my_test_samples_non_covid=os.listdir(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Test/Non-COVID/images/\")\n",
    "    my_test_samples_normal=os.listdir(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Test/Normal/images/\")\n",
    "    for i in my_test_samples_covid_19:\n",
    "        images.append(image_directory+i)\n",
    "    for i in my_test_samples_non_covid:\n",
    "        images.append(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Test/Non-COVID/images/\"+i)\n",
    "    for i in my_test_samples_normal:\n",
    "        images.append(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Test/Normal/images/\"+i)\n",
    "    print(\"appending masks\")\n",
    "    masks=[]\n",
    "    my_test_samples_covid_19=os.listdir(mask_directory)\n",
    "    my_test_samples_non_covid=os.listdir(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Test/Non-COVID/lung masks/\")\n",
    "    my_test_samples_normal=os.listdir(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Test/Normal/lung masks/\")\n",
    "    for i in my_test_samples_covid_19:\n",
    "        masks.append(mask_directory+i)\n",
    "    for i in my_test_samples_non_covid:\n",
    "        masks.append(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Test/Non-COVID/lung masks/\"+i)\n",
    "    for i in my_test_samples_normal:\n",
    "        masks.append(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Test/Normal/lung masks/\"+i)\n",
    "    print(\"shuffling images and masks\")\n",
    "    images=shuffle(images, random_state=0)\n",
    "    masks=shuffle(masks,random_state=0)\n",
    "    for i in images:\n",
    "        x=cv2.imread(i)\n",
    "        i=np.array(x)\n",
    "        if i.shape!=(256,256,3):\n",
    "            print(i.shape)\n",
    "    for i in masks:\n",
    "        x=cv2.imread(i)\n",
    "        i=np.array(x)\n",
    "        if i.shape!=(256,256,3):\n",
    "            print(i.shape)\n",
    "            \n",
    "    print(\"creating image and mask dataset\")\n",
    "    image_dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
    "    mask_dataset = []  #Place holders to define add labels. We will add 0 to all parasitized images and 1 to uninfected.\n",
    "\n",
    "\n",
    "    for i, image_name in enumerate(images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
    "        #if (image_name.split('.')[1] == 'tif'):\n",
    "            #print(image_directory+image_name)\n",
    "\n",
    "        img = Image.open(image_name).convert('L')\n",
    "        image=np.array(img)\n",
    "        #image = image.resize(image,(SIZE, SIZE))[:,:,0]\n",
    "        image_dataset.append(image)\n",
    "\n",
    "    #Iterate through all images in Uninfected folder, resize to 64 x 64\n",
    "    #Then save into the same numpy array 'dataset' but with label 1\n",
    "\n",
    "\n",
    "    for i, image_name in enumerate(masks):\n",
    "        img = Image.open(image_name).convert('L')\n",
    "        image=np.array(img)\n",
    "        #image = image.resize(image,(SIZE, SIZE))[:,:,0]\n",
    "        mask_dataset.append(image)\n",
    "\n",
    "    #Normalize images\n",
    "    print(\"doing expand dims\")\n",
    "    #mage_dataset=np.stack((image_dataset,)*3,axis=-1)\n",
    "    image_dataset = np.array(image_dataset)/255.\n",
    "    test_image_dataset=np.expand_dims((np.array(image_dataset)),3)\n",
    "    #D not normalize masks, just rescale to 0 to 1.\n",
    "    #mask_dataset=np.stack((mask_dataset,)*3,axis=-1)\n",
    "    test_mask_dataset = np.expand_dims((np.array(mask_dataset)),3) /255.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #validation data\n",
    "    image_directory = '/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Val/COVID-19/images/'\n",
    "    mask_directory = '/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Val/COVID-19/lung masks/'\n",
    "    images=[]\n",
    "    my_val_samples_covid_19=os.listdir(image_directory)\n",
    "    my_val_samples_non_covid=os.listdir(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Val/Non-COVID/images/\")\n",
    "    my_val_samples_normal=os.listdir(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Val/Normal/images/\")\n",
    "    for i in my_val_samples_covid_19:\n",
    "        images.append(image_directory+i)\n",
    "    for i in my_val_samples_non_covid:\n",
    "        images.append(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Val/Non-COVID/images/\"+i)\n",
    "    for i in my_val_samples_normal:\n",
    "        images.append(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Val/Normal/images/\"+i)\n",
    "    print(\"appending masks\")\n",
    "    masks=[]\n",
    "    my_val_samples_covid_19=os.listdir(mask_directory)\n",
    "    my_val_samples_non_covid=os.listdir(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Val/Non-COVID/lung masks/\")\n",
    "    my_val_samples_normal=os.listdir(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Val/Normal/lung masks/\")\n",
    "    for i in my_val_samples_covid_19:\n",
    "        masks.append(mask_directory+i)\n",
    "    for i in my_val_samples_non_covid:\n",
    "        masks.append(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Val/Non-COVID/lung masks/\"+i)\n",
    "    for i in my_val_samples_normal:\n",
    "        masks.append(\"/home/pranab_2021cs25/Segmentation_data/Lung_Segmentation_Data/Lung_Segmentation_Data/Val/Normal/lung masks/\"+i)\n",
    "    print(\"shuffling images and masks\")\n",
    "    images=shuffle(images, random_state=0)\n",
    "    masks=shuffle(masks,random_state=0)\n",
    "    for i in images:\n",
    "        x=cv2.imread(i)\n",
    "        i=np.array(x)\n",
    "        if i.shape!=(256,256,3):\n",
    "            print(i.shape)\n",
    "    for i in masks:\n",
    "        x=cv2.imread(i)\n",
    "        i=np.array(x)\n",
    "        if i.shape!=(256,256,3):\n",
    "            print(i.shape)\n",
    "            \n",
    "    print(\"creating image and mask dataset\")\n",
    "    image_dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
    "    mask_dataset = []  #Place holders to define add labels. We will add 0 to all parasitized images and 1 to uninfected.\n",
    "\n",
    "\n",
    "    for i, image_name in enumerate(images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
    "        #if (image_name.split('.')[1] == 'tif'):\n",
    "            #print(image_directory+image_name)\n",
    "\n",
    "        img = Image.open(image_name).convert('L')\n",
    "        image=np.array(img)\n",
    "        #image = image.resize(image,(SIZE, SIZE))[:,:,0]\n",
    "        image_dataset.append(image)\n",
    "\n",
    "    #Iterate through all images in Uninfected folder, resize to 64 x 64\n",
    "    #Then save into the same numpy array 'dataset' but with label 1\n",
    "\n",
    "\n",
    "    for i, image_name in enumerate(masks):\n",
    "        img = Image.open(image_name).convert('L')\n",
    "        image=np.array(img)\n",
    "        #image = image.resize(image,(SIZE, SIZE))[:,:,0]\n",
    "        mask_dataset.append(image)\n",
    "\n",
    "    #Normalize images\n",
    "    print(\"doing expand dims\")\n",
    "    #mage_dataset=np.stack((image_dataset,)*3,axis=-1)\n",
    "    image_dataset = np.array(image_dataset)/255.\n",
    "    val_image_dataset=np.expand_dims((np.array(image_dataset)),3)\n",
    "    #D not normalize masks, just rescale to 0 to 1.\n",
    "    #mask_dataset=np.stack((mask_dataset,)*3,axis=-1)\n",
    "    val_mask_dataset = np.expand_dims((np.array(mask_dataset)),3) /255.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(train_image_dataset.shape)\n",
    "    print(train_mask_dataset.shape)\n",
    "    print(test_image_dataset.shape)\n",
    "    print(test_mask_dataset.shape)\n",
    "    print(val_image_dataset.shape)\n",
    "    print(val_mask_dataset.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "#     print(\"dividing data into train and test split\")\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(image_dataset, mask_dataset, test_size = 0.10, random_state = 0)\n",
    "    IMG_HEIGHT = train_image_dataset.shape[1]\n",
    "    IMG_WIDTH  = train_image_dataset.shape[2]\n",
    "    IMG_CHANNELS = train_image_dataset.shape[3]\n",
    "    num_labels = 1  #Binary\n",
    "    input_shape = (IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS)\n",
    "    batch_size = 32\n",
    "    \n",
    "    print(\"initialize model\")\n",
    "    model_r2_Unet_from_scratch = sm.Unet('vgg16', classes=1, activation='sigmoid',input_shape=(256, 256, 1),encoder_weights='imagenet')\n",
    "#     model_r2_Unet_from_scratch.compile(loss=[dice_coef_loss], optimizer=Adam(lr = 1e-4), \n",
    "#               metrics=[dice_coef, 'binary_accuracy',iou_score])\n",
    "    \n",
    "    model_r2_Unet_from_scratch.compile(optimizer=Adam(lr=1e-4), \n",
    "              loss=[dice_coef_loss], \n",
    "           metrics = [dice_coef, 'binary_accuracy',iou_score])\n",
    "    \n",
    "    start5 = datetime.now() \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                patience=35, \n",
    "\n",
    "                min_delta=0.001, \n",
    "                mode='min')\n",
    "    \n",
    "    print(\"starting fitting\")\n",
    "    r2_Unet_from_scratch_history = model_r2_Unet_from_scratch.fit(train_image_dataset, train_mask_dataset, \n",
    "                        verbose=1,\n",
    "                        batch_size = batch_size,\n",
    "                        validation_data=(val_image_dataset, val_mask_dataset), \n",
    "                        shuffle=False,\n",
    "                        epochs=100,callbacks=[early_stopping])\n",
    "    \n",
    "    history = r2_Unet_from_scratch_history\n",
    "    #np.save('my_history_inceptionv3unet.npy',history.history)\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"traing and validation loss of vgg16_unet.png\")\n",
    "    plt.show()\n",
    "    print(\"second plot\")\n",
    "    acc = history.history['iou_score']\n",
    "    val_acc = history.history['val_iou_score']\n",
    "\n",
    "    plt.plot(epochs, acc, 'y', label='Training IoU')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation IoU')\n",
    "    plt.title('Training and validation IoU')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"training and validation IOU of vgg16_unet.png\")\n",
    "    plt.show()\n",
    "    filename = 'vgg16_unet model.hdf5'\n",
    "    model_r2_Unet_from_scratch.save(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
